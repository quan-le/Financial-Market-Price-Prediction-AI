{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d87cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffa8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "        self.gate = nn.Linear(d_out, d_out)\n",
    "        self.skip = nn.Linear(d_in, d_out) if d_in != d_out else nn.Identity()\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.elu(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        g = torch.sigmoid(self.gate(h))\n",
    "        return self.norm(g * h + (1 - g) * self.skip(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticEncoder(nn.Module):\n",
    "    def __init__(self, d_static, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_static, d_model, d_model)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.grn(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0e2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, num_vars, d_model):\n",
    "        super().__init__()\n",
    "        self.var_grns = nn.ModuleList([\n",
    "            GRN(1, d_model, d_model) for _ in range(num_vars)\n",
    "        ])\n",
    "        self.weight_grn = GRN(num_vars, d_model, num_vars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, num_vars]\n",
    "        var_embeds = []\n",
    "        for i, grn in enumerate(self.var_grns):\n",
    "            var_embeds.append(grn(x[..., i:i+1]))\n",
    "        var_embeds = torch.stack(var_embeds, dim=-2)  # [B,T,num_vars,d]\n",
    "\n",
    "        weights = self.weight_grn(x).softmax(dim=-1)  # [B,T,num_vars]\n",
    "        fused = (weights.unsqueeze(-1) * var_embeds).sum(dim=-2)\n",
    "        return fused, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c454ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEnrichment(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_model * 2, d_model, d_model)\n",
    "\n",
    "    def forward(self, temporal, context):\n",
    "        context = context.unsqueeze(1).expand_as(temporal)\n",
    "        return self.grn(torch.cat([temporal, context], dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79321bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.grn = GRN(d_model, d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "        attn_out, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        out = self.grn(attn_out + x)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1977cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a43b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTFT(nn.Module):\n",
    "    def __init__(self, n_obs, n_known, d_static, d_model=32):\n",
    "        super().__init__()\n",
    "        self.static_enc = StaticEncoder(d_static, d_model)\n",
    "\n",
    "        self.obs_vsn = VariableSelectionNetwork(n_obs, d_model)\n",
    "        self.known_vsn = VariableSelectionNetwork(n_known, d_model)\n",
    "\n",
    "        self.enrich = ContextEnrichment(d_model)\n",
    "        self.attn = TemporalAttention(d_model, num_heads=4)\n",
    "        \n",
    "        self.post_attn_grn = GRN(d_model, d_model, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.head = PredictionHead(d_model)\n",
    "\n",
    "    def forward(self, obs, known, static):\n",
    "        # Preparation for 3 types of inputs\n",
    "        s = self.static_enc(static)\n",
    "        obs_fused,_ = self.obs_vsn(obs)\n",
    "        known_fused,_ = self.known_vsn(known)\n",
    "        # Locality enhanchement\n",
    "        x = obs_fused + known_fused\n",
    "        # Temporal processing\n",
    "        x = self.enrich(x, s)\n",
    "        # Temporal attention\n",
    "        attn_out,attn_weights = self.attn(x)\n",
    "        x = self.layer_norm(attn_out + x)\n",
    "        x = self.layer_norm(self.post_attn_grn(x) + x)\n",
    "        return self.head(x), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62ef317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 819)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 5481 entries, 2010-12-09 to 2025-12-10\n",
      "Columns: 819 entries, open to ret_lag_144\n",
      "dtypes: float64(817), object(2)\n",
      "memory usage: 34.3+ MB\n",
      "None\n",
      "                open      high      low     close    volume symbol  \\\n",
      "date                                                                 \n",
      "2010-12-09 -0.674481 -0.676188 -0.67272 -0.674708 -0.640733    BTC   \n",
      "\n",
      "                   source  building_permits  consumer_confidence       cpi  \\\n",
      "date                                                                         \n",
      "2010-12-09  alpha_vantage         -2.064399            -0.387082 -1.290801   \n",
      "\n",
      "            ...  close_lag_21  ret_lag_21  close_lag_34  ret_lag_34  \\\n",
      "date        ...                                                       \n",
      "2010-12-09  ...     -0.669729    3.274749     -0.667228    2.574162   \n",
      "\n",
      "            close_lag_55  ret_lag_55  close_lag_89  ret_lag_89  close_lag_144  \\\n",
      "date                                                                            \n",
      "2010-12-09     -0.664618    0.563059     -0.662986    0.586438      -0.662793   \n",
      "\n",
      "            ret_lag_144  \n",
      "date                     \n",
      "2010-12-09    11.081548  \n",
      "\n",
      "[1 rows x 819 columns]\n",
      "---------------------------------------\n",
      "['open', 'high', 'low', 'close', 'volume', 'symbol', 'source', 'building_permits', 'consumer_confidence', 'cpi', 'fed_funds_rate', 'gdp', 'industrial_production', 'money_supply_m1', 'money_supply_m2', 'nonfarm_payrolls', 'pce_inflation', 'ppi', 'retail_sales', 'trade_balance', 'unemployment_rate', 'fed_emb_0', 'fed_emb_1', 'fed_emb_2', 'fed_emb_3', 'fed_emb_4', 'fed_emb_5', 'fed_emb_6', 'fed_emb_7', 'fed_emb_8', 'fed_emb_9', 'fed_emb_10', 'fed_emb_11', 'fed_emb_12', 'fed_emb_13', 'fed_emb_14', 'fed_emb_15', 'fed_emb_16', 'fed_emb_17', 'fed_emb_18', 'fed_emb_19', 'fed_emb_20', 'fed_emb_21', 'fed_emb_22', 'fed_emb_23', 'fed_emb_24', 'fed_emb_25', 'fed_emb_26', 'fed_emb_27', 'fed_emb_28', 'fed_emb_29', 'fed_emb_30', 'fed_emb_31', 'fed_emb_32', 'fed_emb_33', 'fed_emb_34', 'fed_emb_35', 'fed_emb_36', 'fed_emb_37', 'fed_emb_38', 'fed_emb_39', 'fed_emb_40', 'fed_emb_41', 'fed_emb_42', 'fed_emb_43', 'fed_emb_44', 'fed_emb_45', 'fed_emb_46', 'fed_emb_47', 'fed_emb_48', 'fed_emb_49', 'fed_emb_50', 'fed_emb_51', 'fed_emb_52', 'fed_emb_53', 'fed_emb_54', 'fed_emb_55', 'fed_emb_56', 'fed_emb_57', 'fed_emb_58', 'fed_emb_59', 'fed_emb_60', 'fed_emb_61', 'fed_emb_62', 'fed_emb_63', 'fed_emb_64', 'fed_emb_65', 'fed_emb_66', 'fed_emb_67', 'fed_emb_68', 'fed_emb_69', 'fed_emb_70', 'fed_emb_71', 'fed_emb_72', 'fed_emb_73', 'fed_emb_74', 'fed_emb_75', 'fed_emb_76', 'fed_emb_77', 'fed_emb_78', 'fed_emb_79', 'fed_emb_80', 'fed_emb_81', 'fed_emb_82', 'fed_emb_83', 'fed_emb_84', 'fed_emb_85', 'fed_emb_86', 'fed_emb_87', 'fed_emb_88', 'fed_emb_89', 'fed_emb_90', 'fed_emb_91', 'fed_emb_92', 'fed_emb_93', 'fed_emb_94', 'fed_emb_95', 'fed_emb_96', 'fed_emb_97', 'fed_emb_98', 'fed_emb_99', 'fed_emb_100', 'fed_emb_101', 'fed_emb_102', 'fed_emb_103', 'fed_emb_104', 'fed_emb_105', 'fed_emb_106', 'fed_emb_107', 'fed_emb_108', 'fed_emb_109', 'fed_emb_110', 'fed_emb_111', 'fed_emb_112', 'fed_emb_113', 'fed_emb_114', 'fed_emb_115', 'fed_emb_116', 'fed_emb_117', 'fed_emb_118', 'fed_emb_119', 'fed_emb_120', 'fed_emb_121', 'fed_emb_122', 'fed_emb_123', 'fed_emb_124', 'fed_emb_125', 'fed_emb_126', 'fed_emb_127', 'fed_emb_128', 'fed_emb_129', 'fed_emb_130', 'fed_emb_131', 'fed_emb_132', 'fed_emb_133', 'fed_emb_134', 'fed_emb_135', 'fed_emb_136', 'fed_emb_137', 'fed_emb_138', 'fed_emb_139', 'fed_emb_140', 'fed_emb_141', 'fed_emb_142', 'fed_emb_143', 'fed_emb_144', 'fed_emb_145', 'fed_emb_146', 'fed_emb_147', 'fed_emb_148', 'fed_emb_149', 'fed_emb_150', 'fed_emb_151', 'fed_emb_152', 'fed_emb_153', 'fed_emb_154', 'fed_emb_155', 'fed_emb_156', 'fed_emb_157', 'fed_emb_158', 'fed_emb_159', 'fed_emb_160', 'fed_emb_161', 'fed_emb_162', 'fed_emb_163', 'fed_emb_164', 'fed_emb_165', 'fed_emb_166', 'fed_emb_167', 'fed_emb_168', 'fed_emb_169', 'fed_emb_170', 'fed_emb_171', 'fed_emb_172', 'fed_emb_173', 'fed_emb_174', 'fed_emb_175', 'fed_emb_176', 'fed_emb_177', 'fed_emb_178', 'fed_emb_179', 'fed_emb_180', 'fed_emb_181', 'fed_emb_182', 'fed_emb_183', 'fed_emb_184', 'fed_emb_185', 'fed_emb_186', 'fed_emb_187', 'fed_emb_188', 'fed_emb_189', 'fed_emb_190', 'fed_emb_191', 'fed_emb_192', 'fed_emb_193', 'fed_emb_194', 'fed_emb_195', 'fed_emb_196', 'fed_emb_197', 'fed_emb_198', 'fed_emb_199', 'fed_emb_200', 'fed_emb_201', 'fed_emb_202', 'fed_emb_203', 'fed_emb_204', 'fed_emb_205', 'fed_emb_206', 'fed_emb_207', 'fed_emb_208', 'fed_emb_209', 'fed_emb_210', 'fed_emb_211', 'fed_emb_212', 'fed_emb_213', 'fed_emb_214', 'fed_emb_215', 'fed_emb_216', 'fed_emb_217', 'fed_emb_218', 'fed_emb_219', 'fed_emb_220', 'fed_emb_221', 'fed_emb_222', 'fed_emb_223', 'fed_emb_224', 'fed_emb_225', 'fed_emb_226', 'fed_emb_227', 'fed_emb_228', 'fed_emb_229', 'fed_emb_230', 'fed_emb_231', 'fed_emb_232', 'fed_emb_233', 'fed_emb_234', 'fed_emb_235', 'fed_emb_236', 'fed_emb_237', 'fed_emb_238', 'fed_emb_239', 'fed_emb_240', 'fed_emb_241', 'fed_emb_242', 'fed_emb_243', 'fed_emb_244', 'fed_emb_245', 'fed_emb_246', 'fed_emb_247', 'fed_emb_248', 'fed_emb_249', 'fed_emb_250', 'fed_emb_251', 'fed_emb_252', 'fed_emb_253', 'fed_emb_254', 'fed_emb_255', 'fed_emb_256', 'fed_emb_257', 'fed_emb_258', 'fed_emb_259', 'fed_emb_260', 'fed_emb_261', 'fed_emb_262', 'fed_emb_263', 'fed_emb_264', 'fed_emb_265', 'fed_emb_266', 'fed_emb_267', 'fed_emb_268', 'fed_emb_269', 'fed_emb_270', 'fed_emb_271', 'fed_emb_272', 'fed_emb_273', 'fed_emb_274', 'fed_emb_275', 'fed_emb_276', 'fed_emb_277', 'fed_emb_278', 'fed_emb_279', 'fed_emb_280', 'fed_emb_281', 'fed_emb_282', 'fed_emb_283', 'fed_emb_284', 'fed_emb_285', 'fed_emb_286', 'fed_emb_287', 'fed_emb_288', 'fed_emb_289', 'fed_emb_290', 'fed_emb_291', 'fed_emb_292', 'fed_emb_293', 'fed_emb_294', 'fed_emb_295', 'fed_emb_296', 'fed_emb_297', 'fed_emb_298', 'fed_emb_299', 'fed_emb_300', 'fed_emb_301', 'fed_emb_302', 'fed_emb_303', 'fed_emb_304', 'fed_emb_305', 'fed_emb_306', 'fed_emb_307', 'fed_emb_308', 'fed_emb_309', 'fed_emb_310', 'fed_emb_311', 'fed_emb_312', 'fed_emb_313', 'fed_emb_314', 'fed_emb_315', 'fed_emb_316', 'fed_emb_317', 'fed_emb_318', 'fed_emb_319', 'fed_emb_320', 'fed_emb_321', 'fed_emb_322', 'fed_emb_323', 'fed_emb_324', 'fed_emb_325', 'fed_emb_326', 'fed_emb_327', 'fed_emb_328', 'fed_emb_329', 'fed_emb_330', 'fed_emb_331', 'fed_emb_332', 'fed_emb_333', 'fed_emb_334', 'fed_emb_335', 'fed_emb_336', 'fed_emb_337', 'fed_emb_338', 'fed_emb_339', 'fed_emb_340', 'fed_emb_341', 'fed_emb_342', 'fed_emb_343', 'fed_emb_344', 'fed_emb_345', 'fed_emb_346', 'fed_emb_347', 'fed_emb_348', 'fed_emb_349', 'fed_emb_350', 'fed_emb_351', 'fed_emb_352', 'fed_emb_353', 'fed_emb_354', 'fed_emb_355', 'fed_emb_356', 'fed_emb_357', 'fed_emb_358', 'fed_emb_359', 'fed_emb_360', 'fed_emb_361', 'fed_emb_362', 'fed_emb_363', 'fed_emb_364', 'fed_emb_365', 'fed_emb_366', 'fed_emb_367', 'fed_emb_368', 'fed_emb_369', 'fed_emb_370', 'fed_emb_371', 'fed_emb_372', 'fed_emb_373', 'fed_emb_374', 'fed_emb_375', 'fed_emb_376', 'fed_emb_377', 'fed_emb_378', 'fed_emb_379', 'fed_emb_380', 'fed_emb_381', 'fed_emb_382', 'fed_emb_383', 'fed_emb_384', 'fed_emb_385', 'fed_emb_386', 'fed_emb_387', 'fed_emb_388', 'fed_emb_389', 'fed_emb_390', 'fed_emb_391', 'fed_emb_392', 'fed_emb_393', 'fed_emb_394', 'fed_emb_395', 'fed_emb_396', 'fed_emb_397', 'fed_emb_398', 'fed_emb_399', 'fed_emb_400', 'fed_emb_401', 'fed_emb_402', 'fed_emb_403', 'fed_emb_404', 'fed_emb_405', 'fed_emb_406', 'fed_emb_407', 'fed_emb_408', 'fed_emb_409', 'fed_emb_410', 'fed_emb_411', 'fed_emb_412', 'fed_emb_413', 'fed_emb_414', 'fed_emb_415', 'fed_emb_416', 'fed_emb_417', 'fed_emb_418', 'fed_emb_419', 'fed_emb_420', 'fed_emb_421', 'fed_emb_422', 'fed_emb_423', 'fed_emb_424', 'fed_emb_425', 'fed_emb_426', 'fed_emb_427', 'fed_emb_428', 'fed_emb_429', 'fed_emb_430', 'fed_emb_431', 'fed_emb_432', 'fed_emb_433', 'fed_emb_434', 'fed_emb_435', 'fed_emb_436', 'fed_emb_437', 'fed_emb_438', 'fed_emb_439', 'fed_emb_440', 'fed_emb_441', 'fed_emb_442', 'fed_emb_443', 'fed_emb_444', 'fed_emb_445', 'fed_emb_446', 'fed_emb_447', 'fed_emb_448', 'fed_emb_449', 'fed_emb_450', 'fed_emb_451', 'fed_emb_452', 'fed_emb_453', 'fed_emb_454', 'fed_emb_455', 'fed_emb_456', 'fed_emb_457', 'fed_emb_458', 'fed_emb_459', 'fed_emb_460', 'fed_emb_461', 'fed_emb_462', 'fed_emb_463', 'fed_emb_464', 'fed_emb_465', 'fed_emb_466', 'fed_emb_467', 'fed_emb_468', 'fed_emb_469', 'fed_emb_470', 'fed_emb_471', 'fed_emb_472', 'fed_emb_473', 'fed_emb_474', 'fed_emb_475', 'fed_emb_476', 'fed_emb_477', 'fed_emb_478', 'fed_emb_479', 'fed_emb_480', 'fed_emb_481', 'fed_emb_482', 'fed_emb_483', 'fed_emb_484', 'fed_emb_485', 'fed_emb_486', 'fed_emb_487', 'fed_emb_488', 'fed_emb_489', 'fed_emb_490', 'fed_emb_491', 'fed_emb_492', 'fed_emb_493', 'fed_emb_494', 'fed_emb_495', 'fed_emb_496', 'fed_emb_497', 'fed_emb_498', 'fed_emb_499', 'fed_emb_500', 'fed_emb_501', 'fed_emb_502', 'fed_emb_503', 'fed_emb_504', 'fed_emb_505', 'fed_emb_506', 'fed_emb_507', 'fed_emb_508', 'fed_emb_509', 'fed_emb_510', 'fed_emb_511', 'fed_emb_512', 'fed_emb_513', 'fed_emb_514', 'fed_emb_515', 'fed_emb_516', 'fed_emb_517', 'fed_emb_518', 'fed_emb_519', 'fed_emb_520', 'fed_emb_521', 'fed_emb_522', 'fed_emb_523', 'fed_emb_524', 'fed_emb_525', 'fed_emb_526', 'fed_emb_527', 'fed_emb_528', 'fed_emb_529', 'fed_emb_530', 'fed_emb_531', 'fed_emb_532', 'fed_emb_533', 'fed_emb_534', 'fed_emb_535', 'fed_emb_536', 'fed_emb_537', 'fed_emb_538', 'fed_emb_539', 'fed_emb_540', 'fed_emb_541', 'fed_emb_542', 'fed_emb_543', 'fed_emb_544', 'fed_emb_545', 'fed_emb_546', 'fed_emb_547', 'fed_emb_548', 'fed_emb_549', 'fed_emb_550', 'fed_emb_551', 'fed_emb_552', 'fed_emb_553', 'fed_emb_554', 'fed_emb_555', 'fed_emb_556', 'fed_emb_557', 'fed_emb_558', 'fed_emb_559', 'fed_emb_560', 'fed_emb_561', 'fed_emb_562', 'fed_emb_563', 'fed_emb_564', 'fed_emb_565', 'fed_emb_566', 'fed_emb_567', 'fed_emb_568', 'fed_emb_569', 'fed_emb_570', 'fed_emb_571', 'fed_emb_572', 'fed_emb_573', 'fed_emb_574', 'fed_emb_575', 'fed_emb_576', 'fed_emb_577', 'fed_emb_578', 'fed_emb_579', 'fed_emb_580', 'fed_emb_581', 'fed_emb_582', 'fed_emb_583', 'fed_emb_584', 'fed_emb_585', 'fed_emb_586', 'fed_emb_587', 'fed_emb_588', 'fed_emb_589', 'fed_emb_590', 'fed_emb_591', 'fed_emb_592', 'fed_emb_593', 'fed_emb_594', 'fed_emb_595', 'fed_emb_596', 'fed_emb_597', 'fed_emb_598', 'fed_emb_599', 'fed_emb_600', 'fed_emb_601', 'fed_emb_602', 'fed_emb_603', 'fed_emb_604', 'fed_emb_605', 'fed_emb_606', 'fed_emb_607', 'fed_emb_608', 'fed_emb_609', 'fed_emb_610', 'fed_emb_611', 'fed_emb_612', 'fed_emb_613', 'fed_emb_614', 'fed_emb_615', 'fed_emb_616', 'fed_emb_617', 'fed_emb_618', 'fed_emb_619', 'fed_emb_620', 'fed_emb_621', 'fed_emb_622', 'fed_emb_623', 'fed_emb_624', 'fed_emb_625', 'fed_emb_626', 'fed_emb_627', 'fed_emb_628', 'fed_emb_629', 'fed_emb_630', 'fed_emb_631', 'fed_emb_632', 'fed_emb_633', 'fed_emb_634', 'fed_emb_635', 'fed_emb_636', 'fed_emb_637', 'fed_emb_638', 'fed_emb_639', 'fed_emb_640', 'fed_emb_641', 'fed_emb_642', 'fed_emb_643', 'fed_emb_644', 'fed_emb_645', 'fed_emb_646', 'fed_emb_647', 'fed_emb_648', 'fed_emb_649', 'fed_emb_650', 'fed_emb_651', 'fed_emb_652', 'fed_emb_653', 'fed_emb_654', 'fed_emb_655', 'fed_emb_656', 'fed_emb_657', 'fed_emb_658', 'fed_emb_659', 'fed_emb_660', 'fed_emb_661', 'fed_emb_662', 'fed_emb_663', 'fed_emb_664', 'fed_emb_665', 'fed_emb_666', 'fed_emb_667', 'fed_emb_668', 'fed_emb_669', 'fed_emb_670', 'fed_emb_671', 'fed_emb_672', 'fed_emb_673', 'fed_emb_674', 'fed_emb_675', 'fed_emb_676', 'fed_emb_677', 'fed_emb_678', 'fed_emb_679', 'fed_emb_680', 'fed_emb_681', 'fed_emb_682', 'fed_emb_683', 'fed_emb_684', 'fed_emb_685', 'fed_emb_686', 'fed_emb_687', 'fed_emb_688', 'fed_emb_689', 'fed_emb_690', 'fed_emb_691', 'fed_emb_692', 'fed_emb_693', 'fed_emb_694', 'fed_emb_695', 'fed_emb_696', 'fed_emb_697', 'fed_emb_698', 'fed_emb_699', 'fed_emb_700', 'fed_emb_701', 'fed_emb_702', 'fed_emb_703', 'fed_emb_704', 'fed_emb_705', 'fed_emb_706', 'fed_emb_707', 'fed_emb_708', 'fed_emb_709', 'fed_emb_710', 'fed_emb_711', 'fed_emb_712', 'fed_emb_713', 'fed_emb_714', 'fed_emb_715', 'fed_emb_716', 'fed_emb_717', 'fed_emb_718', 'fed_emb_719', 'fed_emb_720', 'fed_emb_721', 'fed_emb_722', 'fed_emb_723', 'fed_emb_724', 'fed_emb_725', 'fed_emb_726', 'fed_emb_727', 'fed_emb_728', 'fed_emb_729', 'fed_emb_730', 'fed_emb_731', 'fed_emb_732', 'fed_emb_733', 'fed_emb_734', 'fed_emb_735', 'fed_emb_736', 'fed_emb_737', 'fed_emb_738', 'fed_emb_739', 'fed_emb_740', 'fed_emb_741', 'fed_emb_742', 'fed_emb_743', 'fed_emb_744', 'fed_emb_745', 'fed_emb_746', 'fed_emb_747', 'fed_emb_748', 'fed_emb_749', 'fed_emb_750', 'fed_emb_751', 'fed_emb_752', 'fed_emb_753', 'fed_emb_754', 'fed_emb_755', 'fed_emb_756', 'fed_emb_757', 'fed_emb_758', 'fed_emb_759', 'fed_emb_760', 'fed_emb_761', 'fed_emb_762', 'fed_emb_763', 'fed_emb_764', 'fed_emb_765', 'fed_emb_766', 'fed_emb_767', 'next_close', 'ema_34', 'ema_89', 'ema_200', 'rsi_14', 'macd', 'log_return', 'vol_20', 'close_lag_1', 'ret_lag_1', 'close_lag_2', 'ret_lag_2', 'close_lag_3', 'ret_lag_3', 'close_lag_5', 'ret_lag_5', 'close_lag_8', 'ret_lag_8', 'close_lag_13', 'ret_lag_13', 'close_lag_21', 'ret_lag_21', 'close_lag_34', 'ret_lag_34', 'close_lag_55', 'ret_lag_55', 'close_lag_89', 'ret_lag_89', 'close_lag_144', 'ret_lag_144']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_parquet(\"../../data/features/BTC_features.parquet\")\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head(1))\n",
    "print(\"---------------------------------------\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81e13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\"symbol\", \"source\"]\n",
    "observed_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"ema_34\", \"ema_89\", \"ema_200\",\n",
    "    \"rsi_14\", \"macd\", \"log_return\", \"vol_20\"\n",
    "] + [c for c in df.columns if \"lag_\" in c]\n",
    "known_cols = [\n",
    "    \"building_permits\", \"consumer_confidence\", \"cpi\",\n",
    "    \"fed_funds_rate\", \"gdp\", \"industrial_production\",\n",
    "    \"money_supply_m1\", \"money_supply_m2\",\n",
    "    \"nonfarm_payrolls\", \"pce_inflation\",\n",
    "    \"ppi\", \"retail_sales\", \"trade_balance\",\n",
    "    \"unemployment_rate\"\n",
    "] + [c for c in df.columns if c.startswith(\"fed_emb_\")]\n",
    "target_col = \"next_close\"\n",
    "\n",
    "# train val test splitting\n",
    "n = len(df)\n",
    "train_end = int(0.70 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val   = df.iloc[train_end:val_end]\n",
    "df_test  = df.iloc[val_end:]\n",
    "\n",
    "\n",
    "X_train = df_train[observed_cols + known_cols]\n",
    "X_val   = df_val[observed_cols + known_cols]\n",
    "X_test  = df_test[observed_cols + known_cols]\n",
    "\n",
    "y_train = df_train[target_col].values\n",
    "y_val   = df_val[target_col].values\n",
    "y_test  = df_test[target_col].values\n",
    "# Sliding window\n",
    "LOOKBACK = 89\n",
    "\n",
    "def build_windows(X, y, lookback):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        X_out.append(X[i-lookback:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "Xtr, ytr = build_windows(X_train, y_train, LOOKBACK)\n",
    "Xva, yva = build_windows(X_val, y_val, LOOKBACK)\n",
    "Xte, yte = build_windows(X_test, y_test, LOOKBACK)\n",
    "\n",
    "# split input feature tensors\n",
    "n_obs   = len(observed_cols)\n",
    "n_known = len(known_cols)\n",
    "#Tensor\n",
    "obs_tr   = Xtr[:, :, :n_obs]\n",
    "known_tr = Xtr[:, :, n_obs:]\n",
    "\n",
    "static_tr = np.zeros((len(Xtr), 2))  # placeholder encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874ad131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2,567,785\n",
      "Trainable Parameters: 2,567,785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2567785"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    # Total parameters (including those frozen/not being trained)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Trainable parameters only\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage:\n",
    "model = MiniTFT(n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dedd2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd760d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPu run\n",
    "device = \"cpu\"\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "pred = model(\n",
    "    torch.tensor(obs_tr).float().to(device),\n",
    "    torch.tensor(known_tr).float().to(device),\n",
    "    torch.tensor(static_tr).float().to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d5ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: torch.Size([32, 1])\n",
      "Attention weights shape: torch.Size([32, 89, 89])\n"
     ]
    }
   ],
   "source": [
    "# Testing only \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Try only 32 samples instead of the whole dataset\n",
    "batch_size = 32\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad(): # Use this for testing\n",
    "    pred, weights = model(\n",
    "        torch.tensor(obs_tr[:batch_size]).float().to(device),\n",
    "        torch.tensor(known_tr[:batch_size]).float().to(device),\n",
    "        torch.tensor(static_tr[:batch_size]).float().to(device)\n",
    "    )\n",
    "\n",
    "print(\"Prediction shape:\", pred.shape)\n",
    "print(\"Attention weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36372130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9546],\n",
      "        [1.1523],\n",
      "        [1.0896],\n",
      "        [1.5317],\n",
      "        [1.3048],\n",
      "        [1.2968],\n",
      "        [1.0110],\n",
      "        [1.2259],\n",
      "        [0.8107],\n",
      "        [1.0628],\n",
      "        [1.3527],\n",
      "        [1.4154],\n",
      "        [1.4637],\n",
      "        [1.2682],\n",
      "        [1.2501],\n",
      "        [1.0202],\n",
      "        [1.3022],\n",
      "        [1.1015],\n",
      "        [1.3183],\n",
      "        [1.4186],\n",
      "        [1.3786],\n",
      "        [1.4591],\n",
      "        [1.1272],\n",
      "        [1.2941],\n",
      "        [1.2696],\n",
      "        [1.2421],\n",
      "        [1.2356],\n",
      "        [0.9601],\n",
      "        [1.1149],\n",
      "        [1.2028],\n",
      "        [1.4870],\n",
      "        [1.0974],\n",
      "        [1.1640],\n",
      "        [1.1712],\n",
      "        [1.0972],\n",
      "        [0.9492],\n",
      "        [1.4171],\n",
      "        [1.1475],\n",
      "        [1.0908],\n",
      "        [1.0229],\n",
      "        [1.1764],\n",
      "        [0.7851],\n",
      "        [0.6281],\n",
      "        [0.8995],\n",
      "        [0.7941],\n",
      "        [1.3074],\n",
      "        [1.0451],\n",
      "        [1.1867],\n",
      "        [1.2559],\n",
      "        [1.2965],\n",
      "        [0.5807],\n",
      "        [1.1937],\n",
      "        [1.2392],\n",
      "        [1.0114],\n",
      "        [1.0623],\n",
      "        [1.1461],\n",
      "        [1.2110],\n",
      "        [1.0701],\n",
      "        [0.8695],\n",
      "        [0.9900],\n",
      "        [0.8942],\n",
      "        [1.0765],\n",
      "        [0.9833],\n",
      "        [1.0433]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0890],\n",
      "        [1.3278],\n",
      "        [0.9283],\n",
      "        [1.3783],\n",
      "        [1.1425],\n",
      "        [1.0093],\n",
      "        [1.2504],\n",
      "        [1.2688],\n",
      "        [1.1923],\n",
      "        [1.2438],\n",
      "        [1.2251],\n",
      "        [1.3522],\n",
      "        [0.9818],\n",
      "        [1.0271],\n",
      "        [1.0978],\n",
      "        [1.1389],\n",
      "        [1.2767],\n",
      "        [1.0817],\n",
      "        [1.1349],\n",
      "        [1.1326],\n",
      "        [1.0523],\n",
      "        [1.0980],\n",
      "        [1.1172],\n",
      "        [1.3001],\n",
      "        [1.2373],\n",
      "        [1.1683],\n",
      "        [1.0754],\n",
      "        [1.1364],\n",
      "        [1.0391],\n",
      "        [1.1508],\n",
      "        [1.4537],\n",
      "        [1.3570],\n",
      "        [1.4636],\n",
      "        [1.1048],\n",
      "        [1.1156],\n",
      "        [1.1942],\n",
      "        [1.1991],\n",
      "        [1.3863],\n",
      "        [1.4111],\n",
      "        [1.0319],\n",
      "        [1.1505],\n",
      "        [1.2845],\n",
      "        [1.0858],\n",
      "        [1.3182],\n",
      "        [1.0271],\n",
      "        [1.0074],\n",
      "        [1.4123],\n",
      "        [1.3618],\n",
      "        [1.4560],\n",
      "        [1.2903],\n",
      "        [1.4456],\n",
      "        [1.1863],\n",
      "        [1.0710],\n",
      "        [1.3647],\n",
      "        [1.3921],\n",
      "        [0.8526],\n",
      "        [1.3318],\n",
      "        [1.1743],\n",
      "        [1.2209],\n",
      "        [1.2065],\n",
      "        [0.8470],\n",
      "        [1.2121],\n",
      "        [1.2880],\n",
      "        [1.1359]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.5517],\n",
      "        [1.4042],\n",
      "        [1.3663],\n",
      "        [1.3231],\n",
      "        [1.2907],\n",
      "        [1.1520],\n",
      "        [1.2035],\n",
      "        [1.3962],\n",
      "        [1.2668],\n",
      "        [1.3224],\n",
      "        [1.0231],\n",
      "        [1.1397],\n",
      "        [1.0339],\n",
      "        [0.8634],\n",
      "        [1.3911],\n",
      "        [1.0849],\n",
      "        [1.2399],\n",
      "        [1.1289],\n",
      "        [0.8974],\n",
      "        [1.1150],\n",
      "        [1.2398],\n",
      "        [1.3620],\n",
      "        [0.9917],\n",
      "        [1.0330],\n",
      "        [1.0887],\n",
      "        [1.2110],\n",
      "        [1.1870],\n",
      "        [1.3606],\n",
      "        [1.1643],\n",
      "        [1.1573],\n",
      "        [0.8685],\n",
      "        [1.1102],\n",
      "        [0.8895],\n",
      "        [0.9368],\n",
      "        [1.4109],\n",
      "        [1.2620],\n",
      "        [1.4779],\n",
      "        [0.8406],\n",
      "        [1.3379],\n",
      "        [1.2770],\n",
      "        [1.0404],\n",
      "        [0.8005],\n",
      "        [1.1936],\n",
      "        [1.2580],\n",
      "        [1.4470],\n",
      "        [1.0092],\n",
      "        [0.9450],\n",
      "        [1.1649],\n",
      "        [0.9195],\n",
      "        [1.1512],\n",
      "        [1.2263],\n",
      "        [1.0191],\n",
      "        [1.0532],\n",
      "        [1.5206],\n",
      "        [1.0555],\n",
      "        [1.3573],\n",
      "        [1.1710],\n",
      "        [1.1201],\n",
      "        [1.2686],\n",
      "        [1.0162],\n",
      "        [1.1709],\n",
      "        [1.3439],\n",
      "        [0.6825],\n",
      "        [1.3364]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3861],\n",
      "        [1.0141],\n",
      "        [1.4320],\n",
      "        [1.0930],\n",
      "        [0.9784],\n",
      "        [1.1927],\n",
      "        [1.2704],\n",
      "        [1.2469],\n",
      "        [1.1465],\n",
      "        [1.2728],\n",
      "        [1.0459],\n",
      "        [1.2046],\n",
      "        [1.2492],\n",
      "        [0.9437],\n",
      "        [1.0104],\n",
      "        [1.3795],\n",
      "        [1.0572],\n",
      "        [1.2150],\n",
      "        [1.2296],\n",
      "        [1.0750],\n",
      "        [1.2990],\n",
      "        [1.1917],\n",
      "        [1.4330],\n",
      "        [0.9299],\n",
      "        [1.0822],\n",
      "        [0.9721],\n",
      "        [1.2641],\n",
      "        [0.8297],\n",
      "        [1.2798],\n",
      "        [1.3205],\n",
      "        [1.0825],\n",
      "        [1.3072],\n",
      "        [1.0401],\n",
      "        [1.2230],\n",
      "        [1.0509],\n",
      "        [1.0889],\n",
      "        [0.8980],\n",
      "        [1.4664],\n",
      "        [1.2350],\n",
      "        [1.4003],\n",
      "        [1.3192],\n",
      "        [1.0535],\n",
      "        [1.0913],\n",
      "        [1.0691],\n",
      "        [1.1468],\n",
      "        [1.0764],\n",
      "        [1.3643],\n",
      "        [1.0879],\n",
      "        [1.2169],\n",
      "        [1.1623],\n",
      "        [1.0087],\n",
      "        [1.1744],\n",
      "        [1.1511],\n",
      "        [1.2113],\n",
      "        [1.4234],\n",
      "        [1.1372],\n",
      "        [1.2924],\n",
      "        [1.3321],\n",
      "        [1.0711],\n",
      "        [1.3651],\n",
      "        [1.1928],\n",
      "        [1.2235],\n",
      "        [1.3655],\n",
      "        [0.5729]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1768],\n",
      "        [1.2252],\n",
      "        [1.0980],\n",
      "        [1.2673],\n",
      "        [1.1280],\n",
      "        [1.1550],\n",
      "        [1.1457],\n",
      "        [1.1850],\n",
      "        [0.9342],\n",
      "        [1.0879],\n",
      "        [1.0068],\n",
      "        [1.3556],\n",
      "        [1.3860],\n",
      "        [1.1988],\n",
      "        [0.7372],\n",
      "        [1.3024],\n",
      "        [1.3318],\n",
      "        [1.4262],\n",
      "        [1.1353],\n",
      "        [1.0879],\n",
      "        [0.9037],\n",
      "        [1.2308],\n",
      "        [0.9275],\n",
      "        [1.1479],\n",
      "        [1.3173],\n",
      "        [1.2124],\n",
      "        [1.0688],\n",
      "        [1.0877],\n",
      "        [1.3974],\n",
      "        [1.3356],\n",
      "        [1.1925],\n",
      "        [1.0643],\n",
      "        [1.2492],\n",
      "        [1.3944],\n",
      "        [1.1456],\n",
      "        [1.3745],\n",
      "        [1.0713],\n",
      "        [1.1440],\n",
      "        [0.8343],\n",
      "        [1.0009],\n",
      "        [1.0928],\n",
      "        [1.0740],\n",
      "        [0.9664],\n",
      "        [1.2987],\n",
      "        [1.0507],\n",
      "        [1.1751],\n",
      "        [0.9839],\n",
      "        [1.0037],\n",
      "        [1.2509],\n",
      "        [1.0677],\n",
      "        [1.1232],\n",
      "        [1.2881],\n",
      "        [1.0945],\n",
      "        [1.1761],\n",
      "        [1.1584],\n",
      "        [1.1175],\n",
      "        [0.9659],\n",
      "        [1.4129],\n",
      "        [1.3778],\n",
      "        [1.3510],\n",
      "        [1.0536],\n",
      "        [1.3221],\n",
      "        [1.2097],\n",
      "        [1.1304]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1032],\n",
      "        [1.3788],\n",
      "        [1.2889],\n",
      "        [0.9849],\n",
      "        [1.3431],\n",
      "        [1.4040],\n",
      "        [1.2953],\n",
      "        [1.2946],\n",
      "        [0.7392],\n",
      "        [1.3239],\n",
      "        [1.2833],\n",
      "        [1.1834],\n",
      "        [1.0961],\n",
      "        [1.2022],\n",
      "        [1.1767],\n",
      "        [1.2155],\n",
      "        [1.3745],\n",
      "        [1.2746],\n",
      "        [1.2643],\n",
      "        [1.2305],\n",
      "        [0.9949],\n",
      "        [1.0068],\n",
      "        [1.0093],\n",
      "        [1.2307],\n",
      "        [1.0987],\n",
      "        [1.2501],\n",
      "        [1.3925],\n",
      "        [1.2709],\n",
      "        [1.1113],\n",
      "        [0.7073],\n",
      "        [1.0588],\n",
      "        [1.2168],\n",
      "        [0.6265],\n",
      "        [1.2102],\n",
      "        [1.2195],\n",
      "        [0.5187],\n",
      "        [1.1027],\n",
      "        [1.2581],\n",
      "        [0.8333],\n",
      "        [1.4479],\n",
      "        [1.0455],\n",
      "        [1.0671],\n",
      "        [1.1042],\n",
      "        [1.2964],\n",
      "        [1.1802],\n",
      "        [0.9956],\n",
      "        [1.2520],\n",
      "        [1.2503],\n",
      "        [1.1637],\n",
      "        [1.1766],\n",
      "        [0.8015],\n",
      "        [1.0909],\n",
      "        [1.2572],\n",
      "        [0.9828],\n",
      "        [0.8592],\n",
      "        [1.1260],\n",
      "        [1.2091],\n",
      "        [1.2562],\n",
      "        [1.1379],\n",
      "        [1.0746],\n",
      "        [1.1192],\n",
      "        [1.2153],\n",
      "        [0.9100],\n",
      "        [1.2380]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3230],\n",
      "        [0.9348],\n",
      "        [1.0994],\n",
      "        [0.8151],\n",
      "        [1.1812],\n",
      "        [1.2623],\n",
      "        [1.0913],\n",
      "        [1.0747],\n",
      "        [1.2972],\n",
      "        [1.2811],\n",
      "        [1.3920],\n",
      "        [1.3157],\n",
      "        [0.9089],\n",
      "        [1.1211],\n",
      "        [0.7778],\n",
      "        [1.2013],\n",
      "        [1.1994],\n",
      "        [0.8151],\n",
      "        [0.8273],\n",
      "        [1.2826],\n",
      "        [1.1065],\n",
      "        [1.2427],\n",
      "        [1.1305],\n",
      "        [1.0005],\n",
      "        [1.0528],\n",
      "        [1.2603],\n",
      "        [1.0548],\n",
      "        [1.5332],\n",
      "        [1.1401],\n",
      "        [1.2710],\n",
      "        [1.2861],\n",
      "        [1.2538],\n",
      "        [1.1613],\n",
      "        [1.4244],\n",
      "        [1.0898],\n",
      "        [1.0098],\n",
      "        [1.2547],\n",
      "        [1.0909],\n",
      "        [0.9825],\n",
      "        [1.1963],\n",
      "        [0.4411],\n",
      "        [1.1691],\n",
      "        [1.0651],\n",
      "        [1.1768],\n",
      "        [0.9898],\n",
      "        [1.1940],\n",
      "        [1.2665],\n",
      "        [1.1139],\n",
      "        [1.3695],\n",
      "        [1.3658],\n",
      "        [0.8383],\n",
      "        [1.0618],\n",
      "        [1.3155],\n",
      "        [0.8894],\n",
      "        [0.7074],\n",
      "        [1.2438],\n",
      "        [1.2951],\n",
      "        [1.0649],\n",
      "        [1.4053],\n",
      "        [1.1243],\n",
      "        [0.9734],\n",
      "        [1.1136],\n",
      "        [1.5675],\n",
      "        [1.2414]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1184],\n",
      "        [1.0028],\n",
      "        [1.0915],\n",
      "        [1.2362],\n",
      "        [1.1277],\n",
      "        [0.9809],\n",
      "        [1.3389],\n",
      "        [1.3116],\n",
      "        [1.2059],\n",
      "        [1.2537],\n",
      "        [1.4406],\n",
      "        [0.8638],\n",
      "        [0.9027],\n",
      "        [1.3422],\n",
      "        [1.1655],\n",
      "        [1.3740],\n",
      "        [1.1998],\n",
      "        [1.2361],\n",
      "        [1.1480],\n",
      "        [1.0214],\n",
      "        [1.1524],\n",
      "        [1.1058],\n",
      "        [1.1576],\n",
      "        [1.1597],\n",
      "        [0.7724],\n",
      "        [1.3168],\n",
      "        [1.1684],\n",
      "        [1.6292],\n",
      "        [1.4264],\n",
      "        [1.2289],\n",
      "        [1.2930],\n",
      "        [1.1706],\n",
      "        [1.0176],\n",
      "        [1.0671],\n",
      "        [0.7815],\n",
      "        [0.9393],\n",
      "        [1.3371],\n",
      "        [1.1543],\n",
      "        [1.1443],\n",
      "        [1.2257],\n",
      "        [1.0983],\n",
      "        [1.2750],\n",
      "        [1.3276],\n",
      "        [1.1027],\n",
      "        [0.9932],\n",
      "        [1.1103],\n",
      "        [1.1203],\n",
      "        [1.3583],\n",
      "        [1.1546],\n",
      "        [1.1772],\n",
      "        [1.1159],\n",
      "        [1.1130],\n",
      "        [1.4346],\n",
      "        [0.6813],\n",
      "        [1.2876],\n",
      "        [1.0658],\n",
      "        [1.2419],\n",
      "        [1.1613],\n",
      "        [0.6710],\n",
      "        [0.9289],\n",
      "        [0.8788],\n",
      "        [1.1868],\n",
      "        [1.0478],\n",
      "        [0.9038]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3424],\n",
      "        [1.2308],\n",
      "        [1.0617],\n",
      "        [1.3928],\n",
      "        [1.0942],\n",
      "        [0.8812],\n",
      "        [1.1127],\n",
      "        [1.3296],\n",
      "        [1.1934],\n",
      "        [1.2651],\n",
      "        [1.0339],\n",
      "        [0.5731],\n",
      "        [1.1616],\n",
      "        [0.9741],\n",
      "        [1.1959],\n",
      "        [1.0451],\n",
      "        [1.4462],\n",
      "        [1.2860],\n",
      "        [0.7924],\n",
      "        [1.1734],\n",
      "        [0.7209],\n",
      "        [1.2038],\n",
      "        [1.4513],\n",
      "        [1.2659],\n",
      "        [1.1990],\n",
      "        [1.1281],\n",
      "        [1.3024],\n",
      "        [1.3396],\n",
      "        [0.9668],\n",
      "        [1.2400],\n",
      "        [1.2203],\n",
      "        [1.1567],\n",
      "        [1.1889],\n",
      "        [1.1304],\n",
      "        [0.9781],\n",
      "        [0.7787],\n",
      "        [1.1112],\n",
      "        [1.3236],\n",
      "        [1.2866],\n",
      "        [1.4534],\n",
      "        [1.2533],\n",
      "        [1.1918],\n",
      "        [1.4007],\n",
      "        [1.3347],\n",
      "        [0.9348],\n",
      "        [1.5317],\n",
      "        [1.3953],\n",
      "        [1.2358],\n",
      "        [1.1862],\n",
      "        [1.3227],\n",
      "        [1.0692],\n",
      "        [1.2300],\n",
      "        [1.2763],\n",
      "        [1.2861],\n",
      "        [1.1418],\n",
      "        [1.0771],\n",
      "        [1.3868],\n",
      "        [1.2948],\n",
      "        [1.2061],\n",
      "        [1.2225],\n",
      "        [0.5534],\n",
      "        [1.1084],\n",
      "        [1.1303],\n",
      "        [1.2169]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9622],\n",
      "        [0.9070],\n",
      "        [1.4393],\n",
      "        [1.1423],\n",
      "        [1.1846],\n",
      "        [1.0811],\n",
      "        [0.9298],\n",
      "        [1.3419],\n",
      "        [1.1678],\n",
      "        [1.2233],\n",
      "        [1.2885],\n",
      "        [1.1580],\n",
      "        [1.1714],\n",
      "        [0.9373],\n",
      "        [0.4090],\n",
      "        [0.9430],\n",
      "        [0.9621],\n",
      "        [1.4222],\n",
      "        [1.0352],\n",
      "        [1.4403],\n",
      "        [0.8419],\n",
      "        [1.2066],\n",
      "        [1.1324],\n",
      "        [0.9978],\n",
      "        [1.2739],\n",
      "        [1.3529],\n",
      "        [1.2586],\n",
      "        [1.1415],\n",
      "        [1.4141],\n",
      "        [1.3303],\n",
      "        [1.3430],\n",
      "        [1.4411],\n",
      "        [1.2642],\n",
      "        [1.3271],\n",
      "        [1.2373],\n",
      "        [1.2108],\n",
      "        [0.9696],\n",
      "        [1.3942],\n",
      "        [1.1517],\n",
      "        [0.9908],\n",
      "        [1.4549],\n",
      "        [1.2790],\n",
      "        [1.2908],\n",
      "        [1.5136],\n",
      "        [1.3643],\n",
      "        [1.1784],\n",
      "        [1.0609],\n",
      "        [1.0870],\n",
      "        [1.0845],\n",
      "        [1.0695],\n",
      "        [1.3928],\n",
      "        [1.0972],\n",
      "        [0.8150],\n",
      "        [1.2648],\n",
      "        [1.2008],\n",
      "        [1.2021],\n",
      "        [1.4323],\n",
      "        [1.1720],\n",
      "        [1.1103],\n",
      "        [1.1692],\n",
      "        [1.3627],\n",
      "        [1.1882],\n",
      "        [1.2590],\n",
      "        [1.3571]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1786],\n",
      "        [0.8809],\n",
      "        [1.1700],\n",
      "        [0.9477],\n",
      "        [1.1387],\n",
      "        [1.1838],\n",
      "        [1.0760],\n",
      "        [1.1029],\n",
      "        [1.1794],\n",
      "        [1.1840],\n",
      "        [1.2285],\n",
      "        [1.2229],\n",
      "        [1.1027],\n",
      "        [1.1014],\n",
      "        [1.2801],\n",
      "        [1.1664],\n",
      "        [1.0372],\n",
      "        [1.2176],\n",
      "        [1.1786],\n",
      "        [0.5409],\n",
      "        [1.1032],\n",
      "        [1.0640],\n",
      "        [1.3863],\n",
      "        [1.1018],\n",
      "        [1.5368],\n",
      "        [1.2510],\n",
      "        [1.1116],\n",
      "        [1.1682],\n",
      "        [1.2516],\n",
      "        [1.1342],\n",
      "        [0.3073],\n",
      "        [1.3929],\n",
      "        [1.3006],\n",
      "        [1.4318],\n",
      "        [1.4983],\n",
      "        [0.9139],\n",
      "        [1.0377],\n",
      "        [1.2657],\n",
      "        [1.0451],\n",
      "        [0.8976],\n",
      "        [1.3439],\n",
      "        [1.2279],\n",
      "        [0.9973],\n",
      "        [1.2392],\n",
      "        [1.4276],\n",
      "        [1.4316],\n",
      "        [0.7853],\n",
      "        [1.1237],\n",
      "        [1.1086],\n",
      "        [1.0806],\n",
      "        [1.2463],\n",
      "        [1.2348],\n",
      "        [1.1083],\n",
      "        [1.0139],\n",
      "        [1.3570],\n",
      "        [1.2932],\n",
      "        [1.4090],\n",
      "        [1.2050],\n",
      "        [1.1628],\n",
      "        [1.3105],\n",
      "        [1.3742],\n",
      "        [1.0331],\n",
      "        [1.1198],\n",
      "        [1.4000]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.4926],\n",
      "        [1.3112],\n",
      "        [1.3163],\n",
      "        [1.1405],\n",
      "        [1.3431],\n",
      "        [1.0782],\n",
      "        [1.1220],\n",
      "        [1.2626],\n",
      "        [0.9777],\n",
      "        [1.0812],\n",
      "        [1.5314],\n",
      "        [1.1675],\n",
      "        [1.0945],\n",
      "        [0.9925],\n",
      "        [1.0852],\n",
      "        [1.0613],\n",
      "        [1.2040],\n",
      "        [1.2051],\n",
      "        [1.0758],\n",
      "        [1.3096],\n",
      "        [1.1447],\n",
      "        [1.2023],\n",
      "        [0.9356],\n",
      "        [1.1301],\n",
      "        [1.1974],\n",
      "        [1.2184],\n",
      "        [1.2592],\n",
      "        [1.1376],\n",
      "        [1.2917],\n",
      "        [1.0633],\n",
      "        [1.0437],\n",
      "        [1.1323],\n",
      "        [1.3331],\n",
      "        [1.0250],\n",
      "        [1.1662],\n",
      "        [1.3568],\n",
      "        [1.4442],\n",
      "        [1.1588],\n",
      "        [1.3112],\n",
      "        [1.1359],\n",
      "        [1.2445],\n",
      "        [1.0864],\n",
      "        [1.2080],\n",
      "        [1.1947],\n",
      "        [1.0233],\n",
      "        [1.4744],\n",
      "        [1.6848],\n",
      "        [1.1511],\n",
      "        [1.1647],\n",
      "        [1.2752],\n",
      "        [1.1902],\n",
      "        [1.3483],\n",
      "        [1.1214],\n",
      "        [1.1908],\n",
      "        [1.3342],\n",
      "        [1.2250],\n",
      "        [1.1531],\n",
      "        [0.9898],\n",
      "        [1.2573],\n",
      "        [1.2385],\n",
      "        [1.2348],\n",
      "        [1.1277],\n",
      "        [0.9742],\n",
      "        [1.3601]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2591],\n",
      "        [1.1869],\n",
      "        [0.6100],\n",
      "        [1.0832],\n",
      "        [1.1238],\n",
      "        [1.1948],\n",
      "        [1.2725],\n",
      "        [0.9816],\n",
      "        [1.2977],\n",
      "        [1.2738],\n",
      "        [0.9794],\n",
      "        [1.1910],\n",
      "        [1.1835],\n",
      "        [0.9124],\n",
      "        [1.1140],\n",
      "        [1.2851],\n",
      "        [1.2385],\n",
      "        [1.2043],\n",
      "        [1.3314],\n",
      "        [1.2216],\n",
      "        [0.8539],\n",
      "        [1.5155],\n",
      "        [1.1621],\n",
      "        [1.4571],\n",
      "        [0.7304],\n",
      "        [1.1076],\n",
      "        [1.2514],\n",
      "        [1.1343],\n",
      "        [1.1925],\n",
      "        [1.1223],\n",
      "        [1.1799],\n",
      "        [1.3704],\n",
      "        [1.3060],\n",
      "        [0.9426],\n",
      "        [1.2649],\n",
      "        [1.1155],\n",
      "        [1.0778],\n",
      "        [1.2434],\n",
      "        [1.1082],\n",
      "        [1.3906],\n",
      "        [1.1638],\n",
      "        [1.1844],\n",
      "        [1.3593],\n",
      "        [1.0656],\n",
      "        [0.9765],\n",
      "        [1.3291],\n",
      "        [0.9570],\n",
      "        [0.9947],\n",
      "        [1.3604],\n",
      "        [1.0422],\n",
      "        [0.8854],\n",
      "        [1.2245],\n",
      "        [1.0971],\n",
      "        [1.3775],\n",
      "        [1.4678],\n",
      "        [1.3417],\n",
      "        [1.4851],\n",
      "        [1.3387],\n",
      "        [1.2983],\n",
      "        [1.3297],\n",
      "        [1.1690],\n",
      "        [1.3229],\n",
      "        [1.2742],\n",
      "        [0.8790]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.8033],\n",
      "        [1.2411],\n",
      "        [0.9384],\n",
      "        [1.2733],\n",
      "        [1.2522],\n",
      "        [1.3145],\n",
      "        [1.2258],\n",
      "        [1.2051],\n",
      "        [1.0265],\n",
      "        [0.9993],\n",
      "        [1.2471],\n",
      "        [1.3320],\n",
      "        [1.5727],\n",
      "        [1.4769],\n",
      "        [1.3980],\n",
      "        [1.3152],\n",
      "        [0.9063],\n",
      "        [1.1700],\n",
      "        [1.1523],\n",
      "        [1.2628],\n",
      "        [0.8513],\n",
      "        [1.2928],\n",
      "        [1.1756],\n",
      "        [1.2819],\n",
      "        [1.1651],\n",
      "        [1.0850],\n",
      "        [0.8148],\n",
      "        [1.2423],\n",
      "        [1.1311],\n",
      "        [1.0041],\n",
      "        [1.2008],\n",
      "        [1.3670],\n",
      "        [1.1203],\n",
      "        [1.2988],\n",
      "        [1.0985],\n",
      "        [1.2287],\n",
      "        [1.6686],\n",
      "        [1.2065],\n",
      "        [1.2575],\n",
      "        [1.2720],\n",
      "        [1.0016],\n",
      "        [1.2090],\n",
      "        [1.1399],\n",
      "        [1.4065],\n",
      "        [1.2946],\n",
      "        [1.1260],\n",
      "        [1.3140],\n",
      "        [1.3052],\n",
      "        [1.2640],\n",
      "        [0.9963],\n",
      "        [1.1960],\n",
      "        [1.2978],\n",
      "        [1.0615],\n",
      "        [1.4418],\n",
      "        [1.0439],\n",
      "        [1.1661],\n",
      "        [1.0701],\n",
      "        [1.2788],\n",
      "        [1.1403],\n",
      "        [1.4661],\n",
      "        [0.8568],\n",
      "        [1.2397],\n",
      "        [0.5940],\n",
      "        [1.0301]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2496],\n",
      "        [1.2301],\n",
      "        [1.1711],\n",
      "        [1.2209],\n",
      "        [1.2099],\n",
      "        [1.1784],\n",
      "        [1.2902],\n",
      "        [1.1475],\n",
      "        [1.1818],\n",
      "        [1.2571],\n",
      "        [0.9322],\n",
      "        [1.1010],\n",
      "        [1.3106],\n",
      "        [1.2197],\n",
      "        [0.9781],\n",
      "        [1.1656],\n",
      "        [0.9201],\n",
      "        [1.1224],\n",
      "        [1.2076],\n",
      "        [0.9449],\n",
      "        [1.0676],\n",
      "        [1.1418],\n",
      "        [1.2878],\n",
      "        [1.3451],\n",
      "        [1.1947],\n",
      "        [1.1192],\n",
      "        [1.1982],\n",
      "        [1.2625],\n",
      "        [1.3632],\n",
      "        [1.1991],\n",
      "        [0.9287],\n",
      "        [1.4869],\n",
      "        [0.9466],\n",
      "        [1.2621],\n",
      "        [1.4744],\n",
      "        [1.4962],\n",
      "        [1.2271],\n",
      "        [1.0452],\n",
      "        [1.1339],\n",
      "        [0.9843],\n",
      "        [1.2534],\n",
      "        [1.4496],\n",
      "        [1.1451],\n",
      "        [1.0337],\n",
      "        [1.1843],\n",
      "        [1.3378],\n",
      "        [0.9086],\n",
      "        [1.2221],\n",
      "        [0.9049],\n",
      "        [1.5097],\n",
      "        [1.0873],\n",
      "        [1.1469],\n",
      "        [1.0680],\n",
      "        [1.0554],\n",
      "        [0.9152],\n",
      "        [1.2823],\n",
      "        [1.2915],\n",
      "        [1.4156],\n",
      "        [1.4176],\n",
      "        [1.1639],\n",
      "        [1.3227],\n",
      "        [0.9761],\n",
      "        [0.9381],\n",
      "        [1.5281]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0596],\n",
      "        [1.0888],\n",
      "        [0.8236],\n",
      "        [1.4117],\n",
      "        [1.5317],\n",
      "        [0.9711],\n",
      "        [1.2727],\n",
      "        [1.2803],\n",
      "        [1.0923],\n",
      "        [1.1976],\n",
      "        [1.2933],\n",
      "        [0.8860],\n",
      "        [0.9457],\n",
      "        [1.2101],\n",
      "        [1.0456],\n",
      "        [1.0988],\n",
      "        [1.1546],\n",
      "        [1.2350],\n",
      "        [0.9328],\n",
      "        [1.3908],\n",
      "        [1.1205],\n",
      "        [1.3495],\n",
      "        [1.1163],\n",
      "        [1.3450],\n",
      "        [1.1636],\n",
      "        [1.5131],\n",
      "        [1.0743],\n",
      "        [1.0708],\n",
      "        [1.0761],\n",
      "        [1.2614],\n",
      "        [1.2075],\n",
      "        [1.5517],\n",
      "        [0.9582],\n",
      "        [1.1406],\n",
      "        [1.2452],\n",
      "        [1.3500],\n",
      "        [1.3827],\n",
      "        [1.1965],\n",
      "        [1.1148],\n",
      "        [1.1281],\n",
      "        [1.2730],\n",
      "        [1.3379],\n",
      "        [1.2423],\n",
      "        [1.2670],\n",
      "        [1.1749],\n",
      "        [1.1605],\n",
      "        [1.0562],\n",
      "        [1.2303],\n",
      "        [1.1754],\n",
      "        [1.2130],\n",
      "        [1.4631],\n",
      "        [1.2062],\n",
      "        [1.4457],\n",
      "        [1.0940],\n",
      "        [1.1105],\n",
      "        [1.2377],\n",
      "        [1.2446],\n",
      "        [1.0602],\n",
      "        [1.2065],\n",
      "        [1.2113],\n",
      "        [1.1714],\n",
      "        [1.0663],\n",
      "        [1.4309],\n",
      "        [1.0199]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0583],\n",
      "        [1.2553],\n",
      "        [1.2836],\n",
      "        [1.5021],\n",
      "        [1.2909],\n",
      "        [1.1689],\n",
      "        [1.1997],\n",
      "        [0.7135],\n",
      "        [1.2286],\n",
      "        [1.2020],\n",
      "        [1.0793],\n",
      "        [1.1402],\n",
      "        [1.2735],\n",
      "        [1.1752],\n",
      "        [0.7931],\n",
      "        [1.3868],\n",
      "        [1.0791],\n",
      "        [1.3771],\n",
      "        [1.2828],\n",
      "        [1.2016],\n",
      "        [1.2797],\n",
      "        [1.1238],\n",
      "        [1.3057],\n",
      "        [1.1551],\n",
      "        [1.0976],\n",
      "        [1.5211],\n",
      "        [0.5396],\n",
      "        [0.8033],\n",
      "        [1.4515],\n",
      "        [1.1160],\n",
      "        [1.6416],\n",
      "        [1.4220],\n",
      "        [0.5155],\n",
      "        [1.4853],\n",
      "        [1.2417],\n",
      "        [1.2244],\n",
      "        [1.4172],\n",
      "        [1.3848],\n",
      "        [1.0617],\n",
      "        [0.9684],\n",
      "        [0.6895],\n",
      "        [1.0770],\n",
      "        [1.3173],\n",
      "        [1.1467],\n",
      "        [1.1432],\n",
      "        [1.0729],\n",
      "        [1.4587],\n",
      "        [0.9244],\n",
      "        [1.4642],\n",
      "        [1.2878],\n",
      "        [1.2851],\n",
      "        [1.4518],\n",
      "        [1.1992],\n",
      "        [1.2517],\n",
      "        [0.7290],\n",
      "        [1.0651],\n",
      "        [1.2122],\n",
      "        [0.9810],\n",
      "        [1.0195],\n",
      "        [1.2658],\n",
      "        [1.0320],\n",
      "        [0.9304],\n",
      "        [1.4853],\n",
      "        [1.1768]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2303],\n",
      "        [1.2109],\n",
      "        [1.3122],\n",
      "        [0.8750],\n",
      "        [1.1239],\n",
      "        [1.3372],\n",
      "        [1.0264],\n",
      "        [1.1212],\n",
      "        [0.7416],\n",
      "        [1.1985],\n",
      "        [1.3578],\n",
      "        [1.4087],\n",
      "        [1.2106],\n",
      "        [1.1987],\n",
      "        [1.1370],\n",
      "        [1.2281],\n",
      "        [1.3194],\n",
      "        [1.1795],\n",
      "        [1.0926],\n",
      "        [1.1943],\n",
      "        [1.0804],\n",
      "        [1.2045],\n",
      "        [1.0788],\n",
      "        [1.0275],\n",
      "        [1.3509],\n",
      "        [1.3290],\n",
      "        [1.1858],\n",
      "        [1.2551],\n",
      "        [1.1547],\n",
      "        [1.2129],\n",
      "        [1.1769],\n",
      "        [1.3271],\n",
      "        [1.2262],\n",
      "        [1.1207],\n",
      "        [1.0356],\n",
      "        [1.1987],\n",
      "        [1.0739],\n",
      "        [1.3443],\n",
      "        [1.1039],\n",
      "        [1.2138],\n",
      "        [1.2128],\n",
      "        [1.4078],\n",
      "        [1.1222],\n",
      "        [1.2268],\n",
      "        [1.2435],\n",
      "        [1.3597],\n",
      "        [1.4904],\n",
      "        [1.1307],\n",
      "        [1.2690],\n",
      "        [0.9857],\n",
      "        [1.2530],\n",
      "        [0.8757],\n",
      "        [1.2027],\n",
      "        [1.1126],\n",
      "        [1.1845],\n",
      "        [1.2471],\n",
      "        [1.4820],\n",
      "        [1.0106],\n",
      "        [1.1123],\n",
      "        [0.9524],\n",
      "        [1.3181],\n",
      "        [1.1722],\n",
      "        [1.2121],\n",
      "        [1.3745]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0919],\n",
      "        [1.3386],\n",
      "        [1.3949],\n",
      "        [1.1387],\n",
      "        [1.4247],\n",
      "        [1.0965],\n",
      "        [1.1025],\n",
      "        [1.0841],\n",
      "        [0.9144],\n",
      "        [1.4034],\n",
      "        [1.1961],\n",
      "        [1.1714],\n",
      "        [1.2629],\n",
      "        [1.3287],\n",
      "        [1.5623],\n",
      "        [0.6591],\n",
      "        [1.0642],\n",
      "        [1.1780],\n",
      "        [1.3328],\n",
      "        [1.0914],\n",
      "        [1.1830],\n",
      "        [1.1969],\n",
      "        [1.1936],\n",
      "        [1.0826],\n",
      "        [1.2281],\n",
      "        [1.0529],\n",
      "        [1.0690],\n",
      "        [1.3096],\n",
      "        [1.2519],\n",
      "        [1.1725],\n",
      "        [1.2301],\n",
      "        [1.0168],\n",
      "        [1.3748],\n",
      "        [1.1598],\n",
      "        [1.1898],\n",
      "        [1.3084],\n",
      "        [1.3517],\n",
      "        [1.0504],\n",
      "        [1.2489],\n",
      "        [1.3584],\n",
      "        [1.2309],\n",
      "        [0.7942],\n",
      "        [0.9027],\n",
      "        [1.0748],\n",
      "        [1.0467],\n",
      "        [1.2260],\n",
      "        [1.1040],\n",
      "        [1.0851],\n",
      "        [1.3552],\n",
      "        [1.0642],\n",
      "        [1.1147],\n",
      "        [1.0311],\n",
      "        [1.0558],\n",
      "        [1.1600],\n",
      "        [1.1132],\n",
      "        [0.8837],\n",
      "        [1.0856],\n",
      "        [1.1067],\n",
      "        [1.1307],\n",
      "        [1.1803],\n",
      "        [0.9899],\n",
      "        [1.2251],\n",
      "        [1.4497],\n",
      "        [1.3973]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.7717],\n",
      "        [1.4676],\n",
      "        [1.3726],\n",
      "        [1.0002],\n",
      "        [1.0180],\n",
      "        [1.2839],\n",
      "        [1.1382],\n",
      "        [1.1325],\n",
      "        [1.1911],\n",
      "        [1.1387],\n",
      "        [1.1846],\n",
      "        [1.2519],\n",
      "        [1.4121],\n",
      "        [1.1884],\n",
      "        [1.1973],\n",
      "        [1.1258],\n",
      "        [1.3664],\n",
      "        [0.8709],\n",
      "        [1.1955],\n",
      "        [1.3276],\n",
      "        [1.1298],\n",
      "        [1.1190],\n",
      "        [1.3780],\n",
      "        [0.7445],\n",
      "        [1.0119],\n",
      "        [1.2621],\n",
      "        [1.1069],\n",
      "        [1.1396],\n",
      "        [1.0446],\n",
      "        [1.3382],\n",
      "        [1.3221],\n",
      "        [1.3534],\n",
      "        [1.4375],\n",
      "        [1.1273],\n",
      "        [1.0820],\n",
      "        [1.0246],\n",
      "        [1.1079],\n",
      "        [1.2582],\n",
      "        [1.1399],\n",
      "        [1.3340],\n",
      "        [1.4137],\n",
      "        [1.1324],\n",
      "        [1.2311],\n",
      "        [1.4266],\n",
      "        [1.3223],\n",
      "        [1.1326],\n",
      "        [1.1601],\n",
      "        [1.3552],\n",
      "        [1.2261],\n",
      "        [1.0698],\n",
      "        [1.4530],\n",
      "        [1.5371],\n",
      "        [1.0503],\n",
      "        [1.2002],\n",
      "        [1.3505],\n",
      "        [1.1787],\n",
      "        [0.8212],\n",
      "        [1.3118],\n",
      "        [1.1149],\n",
      "        [1.4005],\n",
      "        [1.4828],\n",
      "        [0.9699],\n",
      "        [0.9478],\n",
      "        [1.2143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.8198],\n",
      "        [1.3575],\n",
      "        [1.2281],\n",
      "        [1.0975],\n",
      "        [1.3272],\n",
      "        [1.2301],\n",
      "        [1.0093],\n",
      "        [1.2120],\n",
      "        [1.1853],\n",
      "        [0.9196],\n",
      "        [1.3022],\n",
      "        [0.9219],\n",
      "        [1.2379],\n",
      "        [0.6784],\n",
      "        [1.2398],\n",
      "        [1.1479],\n",
      "        [1.0789],\n",
      "        [1.2229],\n",
      "        [0.9853],\n",
      "        [1.2584],\n",
      "        [1.3129],\n",
      "        [1.0669],\n",
      "        [1.1390],\n",
      "        [1.1467],\n",
      "        [1.2483],\n",
      "        [0.8779],\n",
      "        [1.0792],\n",
      "        [1.0737],\n",
      "        [1.2288],\n",
      "        [1.1696],\n",
      "        [1.0465],\n",
      "        [1.2712],\n",
      "        [1.0795],\n",
      "        [1.3137],\n",
      "        [1.2683],\n",
      "        [1.1157],\n",
      "        [1.5107],\n",
      "        [1.4229],\n",
      "        [1.1637],\n",
      "        [1.1129],\n",
      "        [1.5312],\n",
      "        [1.2075],\n",
      "        [1.3914],\n",
      "        [1.3443],\n",
      "        [1.0441],\n",
      "        [1.3354],\n",
      "        [1.3141],\n",
      "        [1.3311],\n",
      "        [0.9908],\n",
      "        [1.2876],\n",
      "        [0.4593],\n",
      "        [1.1132],\n",
      "        [1.1639],\n",
      "        [1.3224],\n",
      "        [1.1731],\n",
      "        [1.4310],\n",
      "        [1.2964],\n",
      "        [1.3197],\n",
      "        [1.2032],\n",
      "        [1.1967],\n",
      "        [1.0081],\n",
      "        [1.2820],\n",
      "        [1.1892],\n",
      "        [0.9255]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0339],\n",
      "        [1.3449],\n",
      "        [0.9820],\n",
      "        [1.2767],\n",
      "        [1.3229],\n",
      "        [0.9713],\n",
      "        [1.1121],\n",
      "        [1.0897],\n",
      "        [1.5540],\n",
      "        [1.4887],\n",
      "        [1.2795],\n",
      "        [0.8622],\n",
      "        [1.1422],\n",
      "        [1.2537],\n",
      "        [1.1644],\n",
      "        [1.2670],\n",
      "        [1.3042],\n",
      "        [1.1247],\n",
      "        [1.0794],\n",
      "        [0.9070],\n",
      "        [1.2416],\n",
      "        [1.1159],\n",
      "        [1.1953],\n",
      "        [1.2171],\n",
      "        [0.8537],\n",
      "        [1.1468],\n",
      "        [1.2713],\n",
      "        [1.3468],\n",
      "        [1.3851],\n",
      "        [1.3662],\n",
      "        [1.5065],\n",
      "        [1.3525],\n",
      "        [1.2558],\n",
      "        [1.0673],\n",
      "        [1.3340],\n",
      "        [1.1049],\n",
      "        [1.2652],\n",
      "        [0.6128],\n",
      "        [1.2776],\n",
      "        [1.2920],\n",
      "        [1.0051],\n",
      "        [0.9738],\n",
      "        [0.6305],\n",
      "        [0.5465],\n",
      "        [1.4328],\n",
      "        [1.3595],\n",
      "        [1.2182],\n",
      "        [1.0231],\n",
      "        [1.3743],\n",
      "        [0.8623],\n",
      "        [1.2590],\n",
      "        [1.1116],\n",
      "        [1.1044],\n",
      "        [1.0789],\n",
      "        [0.9273],\n",
      "        [1.1114],\n",
      "        [1.4384],\n",
      "        [1.3484],\n",
      "        [1.5153],\n",
      "        [1.0655],\n",
      "        [0.9680],\n",
      "        [1.1428],\n",
      "        [1.0263],\n",
      "        [0.8388]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2900],\n",
      "        [1.4370],\n",
      "        [0.8798],\n",
      "        [1.0513],\n",
      "        [1.1240],\n",
      "        [1.1782],\n",
      "        [0.9526],\n",
      "        [1.0775],\n",
      "        [1.1957],\n",
      "        [0.8072],\n",
      "        [1.1035],\n",
      "        [1.2740],\n",
      "        [1.2773],\n",
      "        [1.2876],\n",
      "        [1.1246],\n",
      "        [1.4620],\n",
      "        [1.3170],\n",
      "        [0.8461],\n",
      "        [0.8446],\n",
      "        [1.1657],\n",
      "        [1.2650],\n",
      "        [0.6721],\n",
      "        [1.0472],\n",
      "        [0.9559],\n",
      "        [1.2774],\n",
      "        [0.9905],\n",
      "        [0.8041],\n",
      "        [1.2013],\n",
      "        [0.9739],\n",
      "        [1.1715],\n",
      "        [0.7269],\n",
      "        [1.2599],\n",
      "        [1.1462],\n",
      "        [1.0682],\n",
      "        [1.2007],\n",
      "        [1.4459],\n",
      "        [1.4304],\n",
      "        [1.2303],\n",
      "        [0.8793],\n",
      "        [1.3414],\n",
      "        [1.4122],\n",
      "        [1.3877],\n",
      "        [1.2931],\n",
      "        [1.4124],\n",
      "        [1.1009],\n",
      "        [1.1167],\n",
      "        [0.5808],\n",
      "        [1.0635],\n",
      "        [0.6041],\n",
      "        [1.1627],\n",
      "        [1.1470],\n",
      "        [0.9888],\n",
      "        [1.1677],\n",
      "        [1.5537],\n",
      "        [0.9206],\n",
      "        [1.0884],\n",
      "        [1.2845],\n",
      "        [1.2034],\n",
      "        [1.3640],\n",
      "        [1.1156],\n",
      "        [1.1728],\n",
      "        [1.3340],\n",
      "        [1.0743],\n",
      "        [1.3449]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1738],\n",
      "        [1.1114],\n",
      "        [1.1102],\n",
      "        [0.9596],\n",
      "        [1.2207],\n",
      "        [1.3065],\n",
      "        [1.3939],\n",
      "        [1.3212],\n",
      "        [1.3369],\n",
      "        [1.5206],\n",
      "        [0.8834],\n",
      "        [1.2321],\n",
      "        [1.0405],\n",
      "        [1.2230],\n",
      "        [1.1348],\n",
      "        [1.2653],\n",
      "        [0.9805],\n",
      "        [0.9700],\n",
      "        [1.1826],\n",
      "        [1.4855],\n",
      "        [1.0781],\n",
      "        [1.1994],\n",
      "        [1.3579],\n",
      "        [1.2762],\n",
      "        [1.2901],\n",
      "        [1.0714],\n",
      "        [1.2675],\n",
      "        [1.0466],\n",
      "        [1.3636],\n",
      "        [1.0459],\n",
      "        [1.1887],\n",
      "        [1.3148],\n",
      "        [1.1591],\n",
      "        [1.1526],\n",
      "        [1.1788],\n",
      "        [1.1433],\n",
      "        [0.7121],\n",
      "        [1.2927],\n",
      "        [1.1800],\n",
      "        [1.2368],\n",
      "        [1.0030],\n",
      "        [1.1742],\n",
      "        [0.8734],\n",
      "        [1.3202],\n",
      "        [1.6031],\n",
      "        [1.3722],\n",
      "        [1.0136],\n",
      "        [1.3763],\n",
      "        [1.3810],\n",
      "        [1.0852],\n",
      "        [1.2679],\n",
      "        [1.3144],\n",
      "        [1.1020],\n",
      "        [1.0073],\n",
      "        [1.4808],\n",
      "        [1.1307],\n",
      "        [1.1954],\n",
      "        [1.3178],\n",
      "        [1.1385],\n",
      "        [0.9321],\n",
      "        [0.8319],\n",
      "        [1.4222],\n",
      "        [1.2218],\n",
      "        [1.2840]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.4892],\n",
      "        [1.1789],\n",
      "        [1.2372],\n",
      "        [1.2026],\n",
      "        [0.9805],\n",
      "        [0.9289],\n",
      "        [1.4361],\n",
      "        [1.1217],\n",
      "        [1.1307],\n",
      "        [1.0269],\n",
      "        [1.2460],\n",
      "        [1.3127],\n",
      "        [1.4019],\n",
      "        [0.7373],\n",
      "        [1.1140],\n",
      "        [1.1180],\n",
      "        [1.2970],\n",
      "        [1.4250],\n",
      "        [1.0179],\n",
      "        [0.9709],\n",
      "        [0.8106],\n",
      "        [1.1193],\n",
      "        [1.1827],\n",
      "        [1.2744],\n",
      "        [1.3983],\n",
      "        [1.3205],\n",
      "        [1.1969],\n",
      "        [1.3475],\n",
      "        [1.3098],\n",
      "        [1.1410],\n",
      "        [1.4449],\n",
      "        [1.1519],\n",
      "        [1.4510],\n",
      "        [1.2101],\n",
      "        [1.2997],\n",
      "        [1.2514],\n",
      "        [1.2874],\n",
      "        [1.2524],\n",
      "        [1.1716],\n",
      "        [1.2439],\n",
      "        [1.0441],\n",
      "        [1.2076],\n",
      "        [1.1609],\n",
      "        [1.1582],\n",
      "        [1.1815],\n",
      "        [0.8511],\n",
      "        [0.8240],\n",
      "        [1.1190],\n",
      "        [1.0893],\n",
      "        [1.3151],\n",
      "        [1.2009],\n",
      "        [1.2494],\n",
      "        [1.3166],\n",
      "        [1.1649],\n",
      "        [1.1170],\n",
      "        [1.0902],\n",
      "        [1.4184],\n",
      "        [1.2448],\n",
      "        [1.2447],\n",
      "        [1.2415],\n",
      "        [0.7812],\n",
      "        [1.3208],\n",
      "        [1.2944],\n",
      "        [1.2625]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.5224],\n",
      "        [1.1141],\n",
      "        [1.1728],\n",
      "        [1.5435],\n",
      "        [1.1848],\n",
      "        [1.1106],\n",
      "        [1.4199],\n",
      "        [1.3419],\n",
      "        [1.1562],\n",
      "        [0.9475],\n",
      "        [1.1942],\n",
      "        [1.0603],\n",
      "        [0.9400],\n",
      "        [1.2039],\n",
      "        [1.3462],\n",
      "        [1.1316],\n",
      "        [1.0563],\n",
      "        [1.3631],\n",
      "        [1.1882],\n",
      "        [1.3434],\n",
      "        [0.8234],\n",
      "        [1.0435],\n",
      "        [1.0046],\n",
      "        [1.0943],\n",
      "        [1.0346],\n",
      "        [1.3225],\n",
      "        [1.0225],\n",
      "        [0.8220],\n",
      "        [1.2136],\n",
      "        [1.4666],\n",
      "        [1.3080],\n",
      "        [1.3377],\n",
      "        [1.2762],\n",
      "        [0.9634],\n",
      "        [1.0959],\n",
      "        [1.2355],\n",
      "        [1.2067],\n",
      "        [1.0403],\n",
      "        [1.1824],\n",
      "        [0.9935],\n",
      "        [1.3840],\n",
      "        [1.4355],\n",
      "        [1.2264],\n",
      "        [1.4237],\n",
      "        [1.4297],\n",
      "        [0.7985],\n",
      "        [1.1170],\n",
      "        [1.4089],\n",
      "        [1.1869],\n",
      "        [1.0957],\n",
      "        [1.2468],\n",
      "        [1.3720],\n",
      "        [0.8948],\n",
      "        [1.1973],\n",
      "        [1.4959],\n",
      "        [1.0997],\n",
      "        [1.4940],\n",
      "        [1.1536],\n",
      "        [1.2656],\n",
      "        [1.1418],\n",
      "        [1.1999],\n",
      "        [1.1067],\n",
      "        [1.2319],\n",
      "        [1.2311]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1840],\n",
      "        [1.2660],\n",
      "        [0.9337],\n",
      "        [1.1759],\n",
      "        [1.0861],\n",
      "        [1.5425],\n",
      "        [1.0783],\n",
      "        [1.4415],\n",
      "        [0.9534],\n",
      "        [1.4138],\n",
      "        [1.1575],\n",
      "        [1.3572],\n",
      "        [1.2907],\n",
      "        [1.1943],\n",
      "        [1.2085],\n",
      "        [1.4735],\n",
      "        [1.1333],\n",
      "        [1.2690],\n",
      "        [1.2064],\n",
      "        [1.1527],\n",
      "        [1.0263],\n",
      "        [0.7389],\n",
      "        [1.0287],\n",
      "        [1.5140],\n",
      "        [1.0196],\n",
      "        [0.9611],\n",
      "        [1.2670],\n",
      "        [1.1538],\n",
      "        [1.0559],\n",
      "        [1.0730],\n",
      "        [1.4131],\n",
      "        [0.9045],\n",
      "        [1.0074],\n",
      "        [1.4079],\n",
      "        [1.1874],\n",
      "        [1.2688],\n",
      "        [1.2865],\n",
      "        [1.1236],\n",
      "        [0.7308],\n",
      "        [1.2651],\n",
      "        [1.3541],\n",
      "        [1.4784],\n",
      "        [1.2899],\n",
      "        [0.9549],\n",
      "        [1.3784],\n",
      "        [1.0047],\n",
      "        [1.2985],\n",
      "        [1.3621],\n",
      "        [1.0122],\n",
      "        [1.1264],\n",
      "        [1.0776],\n",
      "        [0.7902],\n",
      "        [1.2608],\n",
      "        [1.1957],\n",
      "        [1.1103],\n",
      "        [1.3391],\n",
      "        [1.2430],\n",
      "        [1.2875],\n",
      "        [0.7044],\n",
      "        [1.1420],\n",
      "        [1.2144],\n",
      "        [1.0338],\n",
      "        [0.9922],\n",
      "        [0.9145]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0530],\n",
      "        [0.9263],\n",
      "        [1.1669],\n",
      "        [1.0413],\n",
      "        [0.8133],\n",
      "        [1.3013],\n",
      "        [1.1434],\n",
      "        [1.4468],\n",
      "        [1.1379],\n",
      "        [1.1493],\n",
      "        [1.1849],\n",
      "        [1.0215],\n",
      "        [1.1746],\n",
      "        [1.0569],\n",
      "        [1.1966],\n",
      "        [1.1958],\n",
      "        [1.3673],\n",
      "        [1.1298],\n",
      "        [0.7012],\n",
      "        [1.6319],\n",
      "        [1.2864],\n",
      "        [1.1568],\n",
      "        [1.2192],\n",
      "        [1.2929],\n",
      "        [1.1099],\n",
      "        [1.0979],\n",
      "        [1.1421],\n",
      "        [0.8992],\n",
      "        [1.1666],\n",
      "        [1.2771],\n",
      "        [1.3117],\n",
      "        [1.4113],\n",
      "        [1.2814],\n",
      "        [1.1149],\n",
      "        [1.2318],\n",
      "        [0.9806],\n",
      "        [1.3653],\n",
      "        [1.1597],\n",
      "        [1.0766],\n",
      "        [0.9468],\n",
      "        [1.1649],\n",
      "        [1.3146],\n",
      "        [1.2048],\n",
      "        [1.3213],\n",
      "        [1.1015],\n",
      "        [1.3635],\n",
      "        [1.1665],\n",
      "        [0.9390],\n",
      "        [1.0569],\n",
      "        [1.2623],\n",
      "        [1.1109],\n",
      "        [1.1650],\n",
      "        [1.4003],\n",
      "        [1.3722],\n",
      "        [1.1875],\n",
      "        [1.1822],\n",
      "        [1.1672],\n",
      "        [0.5726],\n",
      "        [1.1872],\n",
      "        [1.1043],\n",
      "        [1.3630],\n",
      "        [1.2825],\n",
      "        [0.7721],\n",
      "        [1.1757]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3001],\n",
      "        [1.1128],\n",
      "        [1.1624],\n",
      "        [1.3118],\n",
      "        [0.7084],\n",
      "        [1.1513],\n",
      "        [1.2744],\n",
      "        [1.0166],\n",
      "        [1.0521],\n",
      "        [1.5053],\n",
      "        [1.1891],\n",
      "        [1.1075],\n",
      "        [1.1881],\n",
      "        [0.9825],\n",
      "        [1.4018],\n",
      "        [1.2707],\n",
      "        [1.2249],\n",
      "        [1.1788],\n",
      "        [1.1512],\n",
      "        [0.9813],\n",
      "        [1.1311],\n",
      "        [0.9324],\n",
      "        [1.1509],\n",
      "        [1.3434],\n",
      "        [1.5284],\n",
      "        [1.1901],\n",
      "        [1.0252],\n",
      "        [1.1660],\n",
      "        [1.1822],\n",
      "        [1.0705],\n",
      "        [1.1024],\n",
      "        [1.3477],\n",
      "        [1.2668],\n",
      "        [1.1251],\n",
      "        [1.4504],\n",
      "        [1.1637],\n",
      "        [1.4428],\n",
      "        [1.2423],\n",
      "        [0.7269],\n",
      "        [1.3507],\n",
      "        [1.4499],\n",
      "        [1.5861],\n",
      "        [1.1000],\n",
      "        [0.7500],\n",
      "        [1.2719],\n",
      "        [1.4142],\n",
      "        [1.3169],\n",
      "        [1.0163],\n",
      "        [1.1641],\n",
      "        [1.0615],\n",
      "        [0.9113],\n",
      "        [0.8218],\n",
      "        [0.9312],\n",
      "        [1.2669],\n",
      "        [1.1614],\n",
      "        [1.2436],\n",
      "        [0.9680],\n",
      "        [1.0224],\n",
      "        [1.0093],\n",
      "        [0.8763],\n",
      "        [1.4421],\n",
      "        [1.3035],\n",
      "        [1.1320],\n",
      "        [1.0793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3402],\n",
      "        [1.3637],\n",
      "        [1.1187],\n",
      "        [1.4159],\n",
      "        [1.1403],\n",
      "        [1.5022],\n",
      "        [0.9977],\n",
      "        [1.3081],\n",
      "        [1.4189],\n",
      "        [1.2299],\n",
      "        [1.0290],\n",
      "        [1.5169],\n",
      "        [1.2148],\n",
      "        [1.1059],\n",
      "        [1.1050],\n",
      "        [1.2462],\n",
      "        [1.3477],\n",
      "        [1.1393],\n",
      "        [1.2938],\n",
      "        [0.9849],\n",
      "        [1.4056],\n",
      "        [1.0857],\n",
      "        [1.1524],\n",
      "        [1.1214],\n",
      "        [1.4305],\n",
      "        [1.1420],\n",
      "        [1.2167],\n",
      "        [1.3046],\n",
      "        [1.1739],\n",
      "        [0.9552],\n",
      "        [1.0911],\n",
      "        [0.9917],\n",
      "        [1.3738],\n",
      "        [1.3407],\n",
      "        [1.3087],\n",
      "        [1.2339],\n",
      "        [1.5215],\n",
      "        [1.2729],\n",
      "        [0.8633],\n",
      "        [0.8444],\n",
      "        [1.3276],\n",
      "        [1.0541],\n",
      "        [1.3300],\n",
      "        [1.0094],\n",
      "        [1.4273],\n",
      "        [1.0723],\n",
      "        [1.3272],\n",
      "        [1.2743],\n",
      "        [0.8018],\n",
      "        [1.1699],\n",
      "        [1.2254],\n",
      "        [1.3009],\n",
      "        [1.4744],\n",
      "        [1.1686],\n",
      "        [1.1355],\n",
      "        [1.1046],\n",
      "        [1.2471],\n",
      "        [0.9003],\n",
      "        [1.4744],\n",
      "        [1.3100],\n",
      "        [1.0223],\n",
      "        [1.0566],\n",
      "        [1.2273],\n",
      "        [1.3811]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9277],\n",
      "        [1.2987],\n",
      "        [1.3520],\n",
      "        [0.7140],\n",
      "        [1.4030],\n",
      "        [1.1160],\n",
      "        [1.2155],\n",
      "        [1.4106],\n",
      "        [1.0151],\n",
      "        [1.3767],\n",
      "        [1.3084],\n",
      "        [1.2882],\n",
      "        [1.4373],\n",
      "        [1.0905],\n",
      "        [1.3853],\n",
      "        [1.0246],\n",
      "        [1.3050],\n",
      "        [1.0843],\n",
      "        [1.4507],\n",
      "        [1.3543],\n",
      "        [1.2416],\n",
      "        [1.1842],\n",
      "        [1.0540],\n",
      "        [1.0643],\n",
      "        [1.4463],\n",
      "        [0.6264],\n",
      "        [1.4354],\n",
      "        [1.1959],\n",
      "        [1.1760],\n",
      "        [0.9847],\n",
      "        [1.2054],\n",
      "        [1.1941],\n",
      "        [1.3644],\n",
      "        [0.7833],\n",
      "        [1.2358],\n",
      "        [1.1411],\n",
      "        [1.4116],\n",
      "        [1.4370],\n",
      "        [1.2514],\n",
      "        [0.9522],\n",
      "        [1.4054],\n",
      "        [0.7784],\n",
      "        [1.1245],\n",
      "        [1.4730],\n",
      "        [1.1774],\n",
      "        [0.9271],\n",
      "        [1.2212],\n",
      "        [1.0714],\n",
      "        [1.1927],\n",
      "        [1.1557],\n",
      "        [1.0395],\n",
      "        [0.6657],\n",
      "        [1.1574],\n",
      "        [1.1906],\n",
      "        [1.5103],\n",
      "        [1.2202],\n",
      "        [0.8922],\n",
      "        [1.2157],\n",
      "        [1.3383],\n",
      "        [1.1506],\n",
      "        [1.4984],\n",
      "        [1.1045],\n",
      "        [0.9766],\n",
      "        [1.3449]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.7713],\n",
      "        [1.0341],\n",
      "        [0.6628],\n",
      "        [0.8996],\n",
      "        [1.2854],\n",
      "        [1.1813],\n",
      "        [1.1879],\n",
      "        [1.3373],\n",
      "        [1.4520],\n",
      "        [1.2963],\n",
      "        [1.1323],\n",
      "        [1.4191],\n",
      "        [1.1234],\n",
      "        [1.0335],\n",
      "        [1.3917],\n",
      "        [1.3198],\n",
      "        [1.0934],\n",
      "        [0.8617],\n",
      "        [1.2779],\n",
      "        [0.6959],\n",
      "        [1.2823],\n",
      "        [1.1694],\n",
      "        [1.2918],\n",
      "        [1.1743],\n",
      "        [1.3373],\n",
      "        [0.8401],\n",
      "        [0.9972],\n",
      "        [1.5265],\n",
      "        [1.5538],\n",
      "        [1.1562],\n",
      "        [0.8964],\n",
      "        [0.8180],\n",
      "        [1.0847],\n",
      "        [1.3360],\n",
      "        [1.0433],\n",
      "        [1.0404],\n",
      "        [0.9846],\n",
      "        [1.0299],\n",
      "        [0.9096],\n",
      "        [1.1272],\n",
      "        [1.1228],\n",
      "        [1.2958],\n",
      "        [1.4196],\n",
      "        [0.9399],\n",
      "        [1.4591],\n",
      "        [1.2968],\n",
      "        [1.3888],\n",
      "        [1.0171],\n",
      "        [1.1318],\n",
      "        [1.1375],\n",
      "        [1.0332],\n",
      "        [1.1204],\n",
      "        [0.8557],\n",
      "        [1.1047],\n",
      "        [1.2466],\n",
      "        [1.0287],\n",
      "        [1.5403],\n",
      "        [1.1155],\n",
      "        [0.9509],\n",
      "        [1.3226],\n",
      "        [1.0866],\n",
      "        [1.1186],\n",
      "        [1.3224],\n",
      "        [0.9279]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1176],\n",
      "        [1.3499],\n",
      "        [1.3799],\n",
      "        [1.2044],\n",
      "        [1.3034],\n",
      "        [0.9513],\n",
      "        [1.1671],\n",
      "        [1.2045],\n",
      "        [1.2678],\n",
      "        [1.0974],\n",
      "        [1.4444],\n",
      "        [1.1009],\n",
      "        [0.6984],\n",
      "        [1.5190],\n",
      "        [1.1019],\n",
      "        [1.0952],\n",
      "        [1.0674],\n",
      "        [1.3735],\n",
      "        [1.1847],\n",
      "        [1.4289],\n",
      "        [1.2057],\n",
      "        [1.1175],\n",
      "        [1.2707],\n",
      "        [0.9735],\n",
      "        [0.9758],\n",
      "        [1.2448],\n",
      "        [1.2713],\n",
      "        [1.2664],\n",
      "        [1.2216],\n",
      "        [0.9245],\n",
      "        [1.1115],\n",
      "        [1.2254],\n",
      "        [0.8009],\n",
      "        [1.2083],\n",
      "        [0.8225],\n",
      "        [0.7836],\n",
      "        [1.3052],\n",
      "        [0.9773],\n",
      "        [0.9763],\n",
      "        [1.1877],\n",
      "        [1.0541],\n",
      "        [1.0703],\n",
      "        [0.6646],\n",
      "        [1.2419],\n",
      "        [0.8339],\n",
      "        [1.0188],\n",
      "        [0.4413],\n",
      "        [1.2334],\n",
      "        [1.0319],\n",
      "        [1.1562],\n",
      "        [1.4413],\n",
      "        [1.2426],\n",
      "        [1.0286],\n",
      "        [1.1466],\n",
      "        [1.3311],\n",
      "        [1.1620],\n",
      "        [1.4794],\n",
      "        [1.0612],\n",
      "        [0.9720],\n",
      "        [1.3362],\n",
      "        [1.1305],\n",
      "        [0.8365],\n",
      "        [1.3171],\n",
      "        [1.6554]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0893],\n",
      "        [0.7162],\n",
      "        [1.5163],\n",
      "        [1.2856],\n",
      "        [1.2325],\n",
      "        [1.1344],\n",
      "        [1.1663],\n",
      "        [0.6889],\n",
      "        [1.0495],\n",
      "        [1.1215],\n",
      "        [0.4651],\n",
      "        [0.6578],\n",
      "        [0.8655],\n",
      "        [1.2023],\n",
      "        [1.1247],\n",
      "        [1.1321],\n",
      "        [1.0160],\n",
      "        [1.1876],\n",
      "        [1.3110],\n",
      "        [0.7153],\n",
      "        [1.5118],\n",
      "        [1.3477],\n",
      "        [1.1832],\n",
      "        [1.0526],\n",
      "        [1.2940],\n",
      "        [1.3469],\n",
      "        [1.2875],\n",
      "        [1.0353],\n",
      "        [1.1108],\n",
      "        [0.8066],\n",
      "        [1.1644],\n",
      "        [1.0877],\n",
      "        [0.8397],\n",
      "        [1.2038],\n",
      "        [1.2801],\n",
      "        [1.0407],\n",
      "        [1.2426],\n",
      "        [0.6366],\n",
      "        [1.0105],\n",
      "        [1.2185],\n",
      "        [0.9058],\n",
      "        [1.1239],\n",
      "        [1.1349],\n",
      "        [1.2361],\n",
      "        [0.9764],\n",
      "        [1.2268],\n",
      "        [1.3579],\n",
      "        [1.2919],\n",
      "        [0.8796],\n",
      "        [1.0167],\n",
      "        [1.2962],\n",
      "        [1.1754],\n",
      "        [0.9641],\n",
      "        [1.3446],\n",
      "        [1.3230],\n",
      "        [1.3026],\n",
      "        [1.3669],\n",
      "        [1.1596],\n",
      "        [1.0949],\n",
      "        [1.3913],\n",
      "        [1.0457],\n",
      "        [1.4648],\n",
      "        [1.3278],\n",
      "        [1.2725]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9834],\n",
      "        [1.1681],\n",
      "        [0.6673],\n",
      "        [1.0309],\n",
      "        [0.4587],\n",
      "        [1.1488],\n",
      "        [0.9923],\n",
      "        [1.1231],\n",
      "        [1.0699],\n",
      "        [1.1999],\n",
      "        [1.2856],\n",
      "        [1.2584],\n",
      "        [1.0240],\n",
      "        [1.1008],\n",
      "        [1.1983],\n",
      "        [1.2280],\n",
      "        [1.1411],\n",
      "        [0.9914],\n",
      "        [1.2878],\n",
      "        [1.0841],\n",
      "        [1.1849],\n",
      "        [1.2355],\n",
      "        [1.3147],\n",
      "        [1.3152],\n",
      "        [1.0322],\n",
      "        [1.2200],\n",
      "        [1.0959],\n",
      "        [0.7952],\n",
      "        [1.1450],\n",
      "        [1.1155],\n",
      "        [0.8007],\n",
      "        [1.1534],\n",
      "        [1.1647],\n",
      "        [1.4990],\n",
      "        [1.2504],\n",
      "        [1.0732],\n",
      "        [0.6842],\n",
      "        [1.3990],\n",
      "        [1.2853],\n",
      "        [1.2704],\n",
      "        [0.8723],\n",
      "        [1.2845],\n",
      "        [1.2336],\n",
      "        [1.2404],\n",
      "        [1.0690],\n",
      "        [1.2536],\n",
      "        [1.0696],\n",
      "        [0.9362],\n",
      "        [0.7396],\n",
      "        [1.1032],\n",
      "        [1.1996],\n",
      "        [1.3166],\n",
      "        [1.0622],\n",
      "        [0.9544],\n",
      "        [1.1794],\n",
      "        [1.1662],\n",
      "        [1.2427],\n",
      "        [1.0651],\n",
      "        [1.2108],\n",
      "        [1.3842],\n",
      "        [1.2217],\n",
      "        [1.3416],\n",
      "        [1.2366],\n",
      "        [1.4220]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3632],\n",
      "        [1.3865],\n",
      "        [1.0870],\n",
      "        [1.1431],\n",
      "        [1.1940],\n",
      "        [1.1776],\n",
      "        [1.1677],\n",
      "        [1.2812],\n",
      "        [1.3531],\n",
      "        [1.3380],\n",
      "        [1.0229],\n",
      "        [1.1602],\n",
      "        [1.2610],\n",
      "        [0.8556],\n",
      "        [1.4381],\n",
      "        [1.3407],\n",
      "        [1.3979],\n",
      "        [1.4641],\n",
      "        [0.8987],\n",
      "        [1.0074],\n",
      "        [1.0338],\n",
      "        [1.1834],\n",
      "        [1.0965],\n",
      "        [1.0858],\n",
      "        [1.1377],\n",
      "        [1.1144],\n",
      "        [1.2627],\n",
      "        [1.1989],\n",
      "        [1.2605],\n",
      "        [1.2751],\n",
      "        [1.3345],\n",
      "        [0.9924],\n",
      "        [1.0765],\n",
      "        [1.0746],\n",
      "        [1.3528],\n",
      "        [1.1289],\n",
      "        [1.4049],\n",
      "        [1.1671],\n",
      "        [1.2593],\n",
      "        [1.2595],\n",
      "        [1.0308],\n",
      "        [1.1547],\n",
      "        [1.0402],\n",
      "        [1.1374],\n",
      "        [0.9669],\n",
      "        [1.2058],\n",
      "        [0.8467],\n",
      "        [1.3176],\n",
      "        [0.9885],\n",
      "        [1.1287],\n",
      "        [1.0854],\n",
      "        [1.3522],\n",
      "        [1.1755],\n",
      "        [1.2642],\n",
      "        [1.4896],\n",
      "        [1.2854],\n",
      "        [1.4047],\n",
      "        [1.1225],\n",
      "        [1.3353],\n",
      "        [1.1155],\n",
      "        [1.3089],\n",
      "        [1.0382],\n",
      "        [1.3827],\n",
      "        [1.4737]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0390],\n",
      "        [1.2694],\n",
      "        [0.9275],\n",
      "        [1.1448],\n",
      "        [1.0823],\n",
      "        [1.0309],\n",
      "        [1.3002],\n",
      "        [1.2224],\n",
      "        [1.2347],\n",
      "        [1.0977],\n",
      "        [1.0952],\n",
      "        [1.4168],\n",
      "        [0.9929],\n",
      "        [1.3238],\n",
      "        [1.0568],\n",
      "        [1.0615],\n",
      "        [1.0684],\n",
      "        [1.3551],\n",
      "        [1.2006],\n",
      "        [1.2953],\n",
      "        [1.2673],\n",
      "        [1.2608],\n",
      "        [1.2468],\n",
      "        [1.3482],\n",
      "        [1.3363],\n",
      "        [1.2740],\n",
      "        [1.1499],\n",
      "        [1.0543],\n",
      "        [1.4081],\n",
      "        [1.0583],\n",
      "        [1.2993],\n",
      "        [1.5051],\n",
      "        [1.4453],\n",
      "        [1.1374],\n",
      "        [1.4270],\n",
      "        [1.3649],\n",
      "        [1.4775],\n",
      "        [0.7291],\n",
      "        [1.3846],\n",
      "        [1.2220],\n",
      "        [1.1466],\n",
      "        [1.2140],\n",
      "        [1.2798],\n",
      "        [1.1402],\n",
      "        [1.2418],\n",
      "        [1.2676],\n",
      "        [1.3097],\n",
      "        [1.1715],\n",
      "        [1.1641],\n",
      "        [1.1213],\n",
      "        [1.2565],\n",
      "        [1.0106],\n",
      "        [1.4259],\n",
      "        [1.1285],\n",
      "        [1.0372],\n",
      "        [1.1213],\n",
      "        [1.4743],\n",
      "        [0.9606],\n",
      "        [1.2453],\n",
      "        [1.1502],\n",
      "        [1.1726],\n",
      "        [1.1859],\n",
      "        [1.1700],\n",
      "        [1.0286]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0429],\n",
      "        [1.1403],\n",
      "        [1.0351],\n",
      "        [1.2935],\n",
      "        [1.1167],\n",
      "        [0.7388],\n",
      "        [1.1308],\n",
      "        [1.2192],\n",
      "        [0.9840],\n",
      "        [1.3222],\n",
      "        [1.0729],\n",
      "        [0.4498],\n",
      "        [1.0731],\n",
      "        [1.0952],\n",
      "        [1.2668],\n",
      "        [1.3901],\n",
      "        [1.1569],\n",
      "        [1.1187],\n",
      "        [0.9552],\n",
      "        [1.2154],\n",
      "        [1.2936],\n",
      "        [1.0074],\n",
      "        [1.3570],\n",
      "        [0.8703],\n",
      "        [1.1404],\n",
      "        [1.1217],\n",
      "        [1.1090],\n",
      "        [0.9488],\n",
      "        [1.1555],\n",
      "        [1.0401],\n",
      "        [1.1953],\n",
      "        [1.0760],\n",
      "        [1.2689],\n",
      "        [0.7659],\n",
      "        [1.1351],\n",
      "        [1.2446],\n",
      "        [1.1976],\n",
      "        [1.2457],\n",
      "        [1.2947],\n",
      "        [1.1091],\n",
      "        [0.9555],\n",
      "        [1.3589],\n",
      "        [0.9061],\n",
      "        [1.0182],\n",
      "        [1.1032],\n",
      "        [1.2742],\n",
      "        [0.8948],\n",
      "        [1.2327],\n",
      "        [1.1399],\n",
      "        [1.3182],\n",
      "        [1.2298],\n",
      "        [1.1302],\n",
      "        [1.4081],\n",
      "        [1.4022],\n",
      "        [1.4499],\n",
      "        [1.3810],\n",
      "        [1.0744],\n",
      "        [1.4118],\n",
      "        [1.2983],\n",
      "        [0.9596],\n",
      "        [1.0320],\n",
      "        [1.2576],\n",
      "        [1.1272],\n",
      "        [1.0441]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2402],\n",
      "        [1.2015],\n",
      "        [1.2269],\n",
      "        [1.3320],\n",
      "        [0.8980],\n",
      "        [1.1478],\n",
      "        [0.7143],\n",
      "        [1.1540],\n",
      "        [1.0474],\n",
      "        [1.0968],\n",
      "        [1.0091],\n",
      "        [1.1281],\n",
      "        [1.2060],\n",
      "        [1.1017],\n",
      "        [1.3343],\n",
      "        [1.3672],\n",
      "        [1.1883],\n",
      "        [1.3413],\n",
      "        [1.3915],\n",
      "        [1.1312],\n",
      "        [1.1540],\n",
      "        [0.6723],\n",
      "        [1.1374],\n",
      "        [0.9992],\n",
      "        [0.7456],\n",
      "        [1.0843],\n",
      "        [0.9887],\n",
      "        [1.4144],\n",
      "        [1.1014],\n",
      "        [1.0781],\n",
      "        [1.3068],\n",
      "        [1.0891],\n",
      "        [1.1728],\n",
      "        [1.1983],\n",
      "        [1.1809],\n",
      "        [1.2047],\n",
      "        [1.1302],\n",
      "        [1.3445],\n",
      "        [1.1432],\n",
      "        [1.1449],\n",
      "        [1.2165],\n",
      "        [1.2803],\n",
      "        [1.1957],\n",
      "        [1.2912],\n",
      "        [1.3294],\n",
      "        [1.0873],\n",
      "        [0.9234],\n",
      "        [1.0444],\n",
      "        [1.2466],\n",
      "        [1.2705],\n",
      "        [1.2950],\n",
      "        [1.3562],\n",
      "        [1.1565],\n",
      "        [0.9395],\n",
      "        [1.4026],\n",
      "        [0.6302],\n",
      "        [1.3060],\n",
      "        [1.4248],\n",
      "        [1.2032],\n",
      "        [1.1192],\n",
      "        [1.5179],\n",
      "        [1.0660],\n",
      "        [0.8962],\n",
      "        [1.1449]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1250],\n",
      "        [0.9779],\n",
      "        [1.1098],\n",
      "        [1.0627],\n",
      "        [1.0277],\n",
      "        [1.2048],\n",
      "        [1.0423],\n",
      "        [1.1830],\n",
      "        [1.2850],\n",
      "        [1.2371],\n",
      "        [1.5030],\n",
      "        [1.0762],\n",
      "        [1.1902],\n",
      "        [1.3108],\n",
      "        [1.2704],\n",
      "        [0.9921],\n",
      "        [1.3540],\n",
      "        [0.8910],\n",
      "        [1.0955],\n",
      "        [1.3490],\n",
      "        [1.0307],\n",
      "        [0.8514],\n",
      "        [1.2943],\n",
      "        [0.9261],\n",
      "        [1.2317],\n",
      "        [1.1723],\n",
      "        [1.4162],\n",
      "        [1.3124],\n",
      "        [0.8580],\n",
      "        [1.2109],\n",
      "        [1.4064],\n",
      "        [0.8429],\n",
      "        [1.0213],\n",
      "        [1.0692],\n",
      "        [1.3479],\n",
      "        [1.1290],\n",
      "        [0.9805],\n",
      "        [1.0089],\n",
      "        [1.2097],\n",
      "        [1.1881],\n",
      "        [1.1617],\n",
      "        [1.0316],\n",
      "        [1.4024],\n",
      "        [0.8318],\n",
      "        [0.8422],\n",
      "        [1.1286],\n",
      "        [0.9356],\n",
      "        [1.1759],\n",
      "        [1.1580],\n",
      "        [1.3018],\n",
      "        [1.0183],\n",
      "        [1.2767],\n",
      "        [1.1979],\n",
      "        [0.7184],\n",
      "        [1.4286],\n",
      "        [0.9100],\n",
      "        [1.0979],\n",
      "        [1.5052],\n",
      "        [1.4664],\n",
      "        [1.0821],\n",
      "        [1.1528],\n",
      "        [1.1144],\n",
      "        [1.5581],\n",
      "        [1.1601]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3520],\n",
      "        [1.1894],\n",
      "        [1.0394],\n",
      "        [0.9428],\n",
      "        [1.2188],\n",
      "        [1.3661],\n",
      "        [1.1074],\n",
      "        [1.1037],\n",
      "        [1.4477],\n",
      "        [1.0261],\n",
      "        [1.3020],\n",
      "        [1.3465],\n",
      "        [1.2010],\n",
      "        [1.1469],\n",
      "        [1.1117],\n",
      "        [0.7075],\n",
      "        [0.6277],\n",
      "        [1.3891],\n",
      "        [1.2192],\n",
      "        [0.9872],\n",
      "        [1.1844],\n",
      "        [0.9237],\n",
      "        [1.0186],\n",
      "        [1.0933],\n",
      "        [1.1750],\n",
      "        [1.1929],\n",
      "        [1.4540],\n",
      "        [0.9854],\n",
      "        [1.0654],\n",
      "        [1.4103],\n",
      "        [1.1102],\n",
      "        [1.1682],\n",
      "        [1.0368],\n",
      "        [1.4588],\n",
      "        [1.3679],\n",
      "        [1.4621],\n",
      "        [1.1729],\n",
      "        [1.0734],\n",
      "        [1.0947],\n",
      "        [1.1818],\n",
      "        [0.7114],\n",
      "        [1.1526],\n",
      "        [1.1854],\n",
      "        [1.3459],\n",
      "        [1.1077],\n",
      "        [1.5192],\n",
      "        [1.3527],\n",
      "        [1.1790],\n",
      "        [1.5157],\n",
      "        [0.8213],\n",
      "        [0.9528],\n",
      "        [1.3125],\n",
      "        [1.3047],\n",
      "        [0.9314],\n",
      "        [1.1670],\n",
      "        [1.0705],\n",
      "        [1.0980],\n",
      "        [1.4660],\n",
      "        [1.2197],\n",
      "        [1.3320],\n",
      "        [0.8751],\n",
      "        [1.2093],\n",
      "        [1.2381],\n",
      "        [1.1241]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1802],\n",
      "        [1.2456],\n",
      "        [1.2537],\n",
      "        [1.1611],\n",
      "        [1.2085],\n",
      "        [1.4146],\n",
      "        [0.9328],\n",
      "        [1.2073],\n",
      "        [1.3545],\n",
      "        [1.2674],\n",
      "        [1.2956],\n",
      "        [1.2289],\n",
      "        [1.5018],\n",
      "        [1.2429],\n",
      "        [1.3897],\n",
      "        [1.2768],\n",
      "        [1.0499],\n",
      "        [1.0766],\n",
      "        [1.2216],\n",
      "        [1.1546],\n",
      "        [1.5140],\n",
      "        [0.9544],\n",
      "        [1.3200],\n",
      "        [0.8167],\n",
      "        [1.0832],\n",
      "        [1.2522],\n",
      "        [1.0267],\n",
      "        [1.1866],\n",
      "        [1.4302],\n",
      "        [0.9663],\n",
      "        [1.2434],\n",
      "        [1.0926],\n",
      "        [1.5351],\n",
      "        [0.7441],\n",
      "        [1.1049],\n",
      "        [1.3247],\n",
      "        [1.2969],\n",
      "        [1.0135],\n",
      "        [1.0835],\n",
      "        [1.2749],\n",
      "        [1.3320],\n",
      "        [1.2790],\n",
      "        [1.3135],\n",
      "        [1.2470],\n",
      "        [1.3547],\n",
      "        [0.7791],\n",
      "        [1.2217],\n",
      "        [0.9784],\n",
      "        [1.3573],\n",
      "        [0.8013],\n",
      "        [1.1044],\n",
      "        [1.3107],\n",
      "        [1.1154],\n",
      "        [1.1232],\n",
      "        [1.1752],\n",
      "        [1.3856],\n",
      "        [1.4140],\n",
      "        [1.2469],\n",
      "        [1.4433],\n",
      "        [1.3196],\n",
      "        [1.5628],\n",
      "        [0.9750],\n",
      "        [1.0657],\n",
      "        [1.1832]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9466],\n",
      "        [1.5245],\n",
      "        [1.2716],\n",
      "        [1.3580],\n",
      "        [1.1323],\n",
      "        [1.1252],\n",
      "        [1.2945],\n",
      "        [1.3695],\n",
      "        [1.2494],\n",
      "        [1.1771],\n",
      "        [1.3569],\n",
      "        [1.1793],\n",
      "        [1.0406],\n",
      "        [1.4118],\n",
      "        [1.0680],\n",
      "        [1.1947],\n",
      "        [1.1341],\n",
      "        [0.8245],\n",
      "        [0.9421],\n",
      "        [1.1540],\n",
      "        [1.4065],\n",
      "        [0.9480],\n",
      "        [1.2642],\n",
      "        [0.9975],\n",
      "        [1.2057],\n",
      "        [1.2285],\n",
      "        [0.5358],\n",
      "        [1.4923],\n",
      "        [1.1318],\n",
      "        [1.0795],\n",
      "        [0.7384],\n",
      "        [1.1016],\n",
      "        [1.3167],\n",
      "        [1.2248],\n",
      "        [1.4542],\n",
      "        [1.1941],\n",
      "        [0.9792],\n",
      "        [0.9964],\n",
      "        [1.1562],\n",
      "        [1.4523],\n",
      "        [1.2516],\n",
      "        [1.2301],\n",
      "        [1.3176],\n",
      "        [1.0959],\n",
      "        [1.1365],\n",
      "        [1.2327],\n",
      "        [1.3720],\n",
      "        [1.2055],\n",
      "        [1.1569],\n",
      "        [1.2929],\n",
      "        [1.2638],\n",
      "        [1.1209],\n",
      "        [1.0039],\n",
      "        [0.8143],\n",
      "        [1.1823],\n",
      "        [1.2275],\n",
      "        [1.4625],\n",
      "        [1.3914],\n",
      "        [1.5802],\n",
      "        [1.2190],\n",
      "        [1.1893],\n",
      "        [0.9473],\n",
      "        [1.0956],\n",
      "        [1.0128]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1553],\n",
      "        [0.5470],\n",
      "        [1.1004],\n",
      "        [1.3767],\n",
      "        [1.1431],\n",
      "        [1.2132],\n",
      "        [1.3742],\n",
      "        [1.0278],\n",
      "        [0.8173],\n",
      "        [1.4396],\n",
      "        [0.9447],\n",
      "        [1.0834],\n",
      "        [1.1514],\n",
      "        [1.4659],\n",
      "        [1.5407],\n",
      "        [1.0805],\n",
      "        [1.1196],\n",
      "        [0.9701],\n",
      "        [0.9129],\n",
      "        [1.2716],\n",
      "        [1.1976],\n",
      "        [1.2017],\n",
      "        [1.3822],\n",
      "        [1.1902],\n",
      "        [1.2830],\n",
      "        [1.0080],\n",
      "        [1.3211],\n",
      "        [1.1377],\n",
      "        [1.1555],\n",
      "        [1.1919],\n",
      "        [1.4595],\n",
      "        [1.2259],\n",
      "        [0.9520],\n",
      "        [1.2352],\n",
      "        [1.1290],\n",
      "        [1.1121],\n",
      "        [1.1670],\n",
      "        [1.0817],\n",
      "        [1.0668],\n",
      "        [1.2145],\n",
      "        [0.9598],\n",
      "        [1.2473],\n",
      "        [1.0638],\n",
      "        [1.0884],\n",
      "        [0.9579],\n",
      "        [1.1422],\n",
      "        [1.1537],\n",
      "        [1.0144],\n",
      "        [1.2417],\n",
      "        [1.2622],\n",
      "        [1.1512],\n",
      "        [1.2551],\n",
      "        [1.1119],\n",
      "        [1.2962],\n",
      "        [1.1581],\n",
      "        [1.0570],\n",
      "        [1.3271],\n",
      "        [1.1087],\n",
      "        [1.3825],\n",
      "        [0.9195],\n",
      "        [0.8924],\n",
      "        [1.1676],\n",
      "        [0.9841],\n",
      "        [0.8424]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3298],\n",
      "        [1.4002],\n",
      "        [0.9625],\n",
      "        [1.1525],\n",
      "        [1.2719],\n",
      "        [1.2568],\n",
      "        [1.1180],\n",
      "        [1.3314],\n",
      "        [1.2362],\n",
      "        [1.1872],\n",
      "        [1.1885],\n",
      "        [0.7538],\n",
      "        [1.1452],\n",
      "        [0.9050],\n",
      "        [1.1111],\n",
      "        [1.2927],\n",
      "        [1.1370],\n",
      "        [1.1755],\n",
      "        [1.3437],\n",
      "        [0.9325],\n",
      "        [1.0990],\n",
      "        [0.8820],\n",
      "        [1.1437],\n",
      "        [1.3198],\n",
      "        [1.0944],\n",
      "        [1.3705],\n",
      "        [1.3370],\n",
      "        [1.3272],\n",
      "        [1.2014],\n",
      "        [1.2370],\n",
      "        [1.1649],\n",
      "        [1.1313],\n",
      "        [1.0704],\n",
      "        [1.1356],\n",
      "        [1.2900],\n",
      "        [0.7571],\n",
      "        [1.1760],\n",
      "        [1.4502],\n",
      "        [1.2678],\n",
      "        [1.3451],\n",
      "        [1.2807],\n",
      "        [1.2297],\n",
      "        [1.1744],\n",
      "        [1.3727],\n",
      "        [1.2485],\n",
      "        [1.0228],\n",
      "        [0.8692],\n",
      "        [1.1156],\n",
      "        [1.3607],\n",
      "        [1.2975],\n",
      "        [1.0144],\n",
      "        [1.1153],\n",
      "        [1.4728],\n",
      "        [1.2383],\n",
      "        [1.3270],\n",
      "        [0.7357],\n",
      "        [1.1508],\n",
      "        [0.9848],\n",
      "        [1.1935],\n",
      "        [1.3962],\n",
      "        [1.0327],\n",
      "        [1.0633],\n",
      "        [1.0737],\n",
      "        [1.2201]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1543],\n",
      "        [1.1912],\n",
      "        [1.2471],\n",
      "        [1.2083],\n",
      "        [1.1265],\n",
      "        [0.8442],\n",
      "        [0.9246],\n",
      "        [1.3966],\n",
      "        [0.5596],\n",
      "        [1.2123],\n",
      "        [1.3618],\n",
      "        [1.2339],\n",
      "        [1.1905],\n",
      "        [1.1605],\n",
      "        [1.3649],\n",
      "        [1.3727],\n",
      "        [1.3539],\n",
      "        [1.3256],\n",
      "        [1.4911],\n",
      "        [1.1650],\n",
      "        [1.0731],\n",
      "        [1.0712],\n",
      "        [1.2311],\n",
      "        [1.1853],\n",
      "        [1.1667],\n",
      "        [0.9839],\n",
      "        [1.1624],\n",
      "        [1.0147],\n",
      "        [0.5583],\n",
      "        [1.0867],\n",
      "        [1.3976],\n",
      "        [1.4345],\n",
      "        [1.1919],\n",
      "        [1.1357],\n",
      "        [1.1719],\n",
      "        [1.0933],\n",
      "        [0.8016],\n",
      "        [1.2433],\n",
      "        [1.1843],\n",
      "        [1.0892],\n",
      "        [1.2113],\n",
      "        [1.2936],\n",
      "        [1.2486],\n",
      "        [1.3716],\n",
      "        [1.2605],\n",
      "        [1.1227],\n",
      "        [0.9550],\n",
      "        [1.0684],\n",
      "        [1.2093],\n",
      "        [1.1874],\n",
      "        [1.1563],\n",
      "        [1.1140],\n",
      "        [1.4645],\n",
      "        [1.2270],\n",
      "        [1.3821],\n",
      "        [1.3490],\n",
      "        [1.4908],\n",
      "        [1.1630],\n",
      "        [0.9806],\n",
      "        [1.1527],\n",
      "        [1.0010],\n",
      "        [1.4190],\n",
      "        [1.3220],\n",
      "        [1.5153]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2471],\n",
      "        [1.1390],\n",
      "        [1.0312],\n",
      "        [1.3139],\n",
      "        [1.1380],\n",
      "        [1.2542],\n",
      "        [1.3182],\n",
      "        [0.8215],\n",
      "        [1.2568],\n",
      "        [1.1762],\n",
      "        [1.0143],\n",
      "        [1.2399],\n",
      "        [1.0083],\n",
      "        [1.1041],\n",
      "        [1.2366],\n",
      "        [1.3019],\n",
      "        [1.3534],\n",
      "        [1.3931],\n",
      "        [1.1332],\n",
      "        [1.1121],\n",
      "        [1.0594],\n",
      "        [0.9048],\n",
      "        [0.9249],\n",
      "        [1.0268],\n",
      "        [1.1400],\n",
      "        [1.3878],\n",
      "        [1.2940],\n",
      "        [1.4252],\n",
      "        [0.7028],\n",
      "        [1.2371],\n",
      "        [1.2402],\n",
      "        [0.9539],\n",
      "        [1.0753],\n",
      "        [1.0776],\n",
      "        [1.0611],\n",
      "        [1.2733],\n",
      "        [1.3050],\n",
      "        [1.1633],\n",
      "        [0.8023],\n",
      "        [0.8824],\n",
      "        [1.2102],\n",
      "        [1.1786],\n",
      "        [0.7684],\n",
      "        [1.0870],\n",
      "        [1.1917],\n",
      "        [1.3410],\n",
      "        [1.0927],\n",
      "        [1.1229],\n",
      "        [1.2236],\n",
      "        [1.4839],\n",
      "        [1.1202],\n",
      "        [1.1500],\n",
      "        [1.4915],\n",
      "        [1.1937],\n",
      "        [1.4063],\n",
      "        [1.2625],\n",
      "        [1.2661],\n",
      "        [1.2841],\n",
      "        [0.8498],\n",
      "        [0.9309],\n",
      "        [0.6593],\n",
      "        [1.0788],\n",
      "        [1.4719],\n",
      "        [0.9563]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.3987],\n",
      "        [0.9447],\n",
      "        [1.0546],\n",
      "        [1.0925],\n",
      "        [1.1910],\n",
      "        [0.0310],\n",
      "        [0.9601],\n",
      "        [1.0264],\n",
      "        [1.4517],\n",
      "        [1.2926],\n",
      "        [1.1201],\n",
      "        [1.3899],\n",
      "        [1.3576],\n",
      "        [1.2214],\n",
      "        [1.1562],\n",
      "        [1.2151],\n",
      "        [1.0767],\n",
      "        [1.0580],\n",
      "        [1.1142],\n",
      "        [0.8750],\n",
      "        [0.5796],\n",
      "        [1.2197],\n",
      "        [1.3893],\n",
      "        [1.0076],\n",
      "        [1.2659],\n",
      "        [1.1486],\n",
      "        [1.0020],\n",
      "        [1.4040],\n",
      "        [1.2761],\n",
      "        [1.3850],\n",
      "        [1.2855],\n",
      "        [0.8396],\n",
      "        [1.2538],\n",
      "        [1.1026],\n",
      "        [1.1551],\n",
      "        [1.2855],\n",
      "        [0.9919],\n",
      "        [1.0500],\n",
      "        [1.0796],\n",
      "        [0.8987],\n",
      "        [1.2458],\n",
      "        [1.4725],\n",
      "        [1.1443],\n",
      "        [1.2914],\n",
      "        [1.3146],\n",
      "        [1.4152],\n",
      "        [1.1044],\n",
      "        [1.5068],\n",
      "        [1.1554],\n",
      "        [1.3894],\n",
      "        [1.5163],\n",
      "        [0.8497],\n",
      "        [1.2421],\n",
      "        [0.9321],\n",
      "        [1.1803],\n",
      "        [1.2443],\n",
      "        [1.2366],\n",
      "        [1.1593],\n",
      "        [1.3627],\n",
      "        [1.2017],\n",
      "        [1.2893],\n",
      "        [1.4909],\n",
      "        [1.0239],\n",
      "        [1.0484]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0267],\n",
      "        [1.1624],\n",
      "        [1.5381],\n",
      "        [1.2263],\n",
      "        [1.0329],\n",
      "        [1.2301],\n",
      "        [0.7661],\n",
      "        [1.1801],\n",
      "        [1.3761],\n",
      "        [1.2187],\n",
      "        [1.0229],\n",
      "        [0.5100],\n",
      "        [0.8618],\n",
      "        [1.1947],\n",
      "        [1.2986],\n",
      "        [1.2973],\n",
      "        [1.0638],\n",
      "        [1.1730],\n",
      "        [1.2290],\n",
      "        [1.4055],\n",
      "        [1.3672],\n",
      "        [1.1433],\n",
      "        [1.3489],\n",
      "        [1.2004],\n",
      "        [1.4579],\n",
      "        [1.1522],\n",
      "        [1.0789],\n",
      "        [1.1645],\n",
      "        [1.2286],\n",
      "        [1.0619],\n",
      "        [1.2568],\n",
      "        [1.3844],\n",
      "        [1.2592],\n",
      "        [1.0058],\n",
      "        [1.3362],\n",
      "        [1.2530],\n",
      "        [0.9640],\n",
      "        [0.7864],\n",
      "        [0.9346],\n",
      "        [1.2833],\n",
      "        [1.2339],\n",
      "        [0.9422],\n",
      "        [1.2354],\n",
      "        [0.7674],\n",
      "        [1.1079],\n",
      "        [0.8735],\n",
      "        [1.3844],\n",
      "        [1.4260],\n",
      "        [1.3265],\n",
      "        [1.2207],\n",
      "        [0.9492],\n",
      "        [1.1141],\n",
      "        [1.4078],\n",
      "        [1.1900],\n",
      "        [1.0497],\n",
      "        [1.0549],\n",
      "        [1.1532],\n",
      "        [1.3919],\n",
      "        [1.2689],\n",
      "        [1.1901],\n",
      "        [1.2618],\n",
      "        [1.0487],\n",
      "        [1.1916],\n",
      "        [0.9759]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.1534],\n",
      "        [1.2625],\n",
      "        [1.3367],\n",
      "        [1.1347],\n",
      "        [0.8952],\n",
      "        [0.9938],\n",
      "        [0.7423],\n",
      "        [1.3884],\n",
      "        [1.4166],\n",
      "        [1.0665],\n",
      "        [1.6971],\n",
      "        [1.2414],\n",
      "        [1.2633],\n",
      "        [1.1629],\n",
      "        [1.2117],\n",
      "        [1.1037],\n",
      "        [1.1748],\n",
      "        [1.2621],\n",
      "        [1.2029],\n",
      "        [1.1082],\n",
      "        [1.1562],\n",
      "        [0.8264],\n",
      "        [1.2737],\n",
      "        [1.2150],\n",
      "        [0.9162],\n",
      "        [1.2991],\n",
      "        [1.2516],\n",
      "        [1.2789],\n",
      "        [1.1740],\n",
      "        [1.1149],\n",
      "        [1.5045],\n",
      "        [1.2439],\n",
      "        [1.1548],\n",
      "        [1.1507],\n",
      "        [1.3463],\n",
      "        [1.4063],\n",
      "        [0.8500],\n",
      "        [0.9755],\n",
      "        [1.4370],\n",
      "        [0.4783],\n",
      "        [1.0637],\n",
      "        [1.2844],\n",
      "        [1.1274],\n",
      "        [0.8182],\n",
      "        [1.2951],\n",
      "        [1.5056],\n",
      "        [1.2135],\n",
      "        [0.9416],\n",
      "        [1.3570],\n",
      "        [0.7895],\n",
      "        [0.5895],\n",
      "        [0.9847],\n",
      "        [1.1644],\n",
      "        [1.1845],\n",
      "        [0.9368],\n",
      "        [1.0363],\n",
      "        [1.1566],\n",
      "        [1.2123],\n",
      "        [1.3530],\n",
      "        [1.1966],\n",
      "        [1.1845],\n",
      "        [1.4885],\n",
      "        [1.3688],\n",
      "        [1.1374]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.8327],\n",
      "        [1.4664],\n",
      "        [1.2823],\n",
      "        [1.1415],\n",
      "        [1.2956],\n",
      "        [1.2591],\n",
      "        [1.2895],\n",
      "        [1.2539],\n",
      "        [1.2490],\n",
      "        [1.3938],\n",
      "        [0.8815],\n",
      "        [0.9164],\n",
      "        [1.1757],\n",
      "        [0.6333],\n",
      "        [1.2208],\n",
      "        [1.2079],\n",
      "        [1.2846],\n",
      "        [1.2049],\n",
      "        [1.0964],\n",
      "        [1.1313],\n",
      "        [1.4297],\n",
      "        [1.4117],\n",
      "        [1.1995],\n",
      "        [1.3109],\n",
      "        [1.4570],\n",
      "        [1.2600],\n",
      "        [1.2602],\n",
      "        [0.8189],\n",
      "        [1.0540],\n",
      "        [0.7742],\n",
      "        [1.3164],\n",
      "        [1.1153],\n",
      "        [1.1501],\n",
      "        [1.2852],\n",
      "        [0.5155],\n",
      "        [1.1360],\n",
      "        [1.2384],\n",
      "        [1.4509],\n",
      "        [1.3326],\n",
      "        [1.2648],\n",
      "        [1.0618],\n",
      "        [1.3717],\n",
      "        [1.3629],\n",
      "        [1.1436],\n",
      "        [1.3439],\n",
      "        [0.6521],\n",
      "        [0.8918],\n",
      "        [1.1688],\n",
      "        [1.2866],\n",
      "        [1.0322],\n",
      "        [0.4953],\n",
      "        [0.9963],\n",
      "        [1.2225],\n",
      "        [0.7886],\n",
      "        [0.8825],\n",
      "        [1.0259],\n",
      "        [1.2134],\n",
      "        [1.4777],\n",
      "        [0.6808],\n",
      "        [1.2922],\n",
      "        [0.9579],\n",
      "        [1.3095],\n",
      "        [0.9272],\n",
      "        [0.7639]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2292],\n",
      "        [0.9681],\n",
      "        [0.8955],\n",
      "        [1.0947],\n",
      "        [1.3234],\n",
      "        [1.3228],\n",
      "        [1.2398],\n",
      "        [1.3070],\n",
      "        [0.3751],\n",
      "        [1.2775],\n",
      "        [1.1550],\n",
      "        [1.3483],\n",
      "        [1.1145],\n",
      "        [0.9649],\n",
      "        [1.2801],\n",
      "        [1.1620],\n",
      "        [1.0839],\n",
      "        [1.3265],\n",
      "        [1.4118],\n",
      "        [1.2809],\n",
      "        [0.7807],\n",
      "        [1.1385],\n",
      "        [1.2007],\n",
      "        [1.3665],\n",
      "        [1.1325],\n",
      "        [0.8128],\n",
      "        [1.5029],\n",
      "        [1.3835],\n",
      "        [0.6186],\n",
      "        [1.1362],\n",
      "        [1.3063],\n",
      "        [1.3691],\n",
      "        [1.5034],\n",
      "        [1.4731],\n",
      "        [1.2379],\n",
      "        [1.3249],\n",
      "        [1.5928],\n",
      "        [1.3328],\n",
      "        [1.1943],\n",
      "        [1.0970],\n",
      "        [1.1209],\n",
      "        [0.9749],\n",
      "        [1.5499],\n",
      "        [1.2937],\n",
      "        [1.2042],\n",
      "        [1.0389],\n",
      "        [1.0567],\n",
      "        [1.5090],\n",
      "        [1.1620],\n",
      "        [1.5464],\n",
      "        [1.2173],\n",
      "        [1.1896],\n",
      "        [1.1401],\n",
      "        [1.0314],\n",
      "        [0.9532],\n",
      "        [1.2095],\n",
      "        [1.0283],\n",
      "        [1.2482],\n",
      "        [0.9278],\n",
      "        [1.0169],\n",
      "        [1.3620],\n",
      "        [1.0219],\n",
      "        [1.2002],\n",
      "        [1.0500]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.2262],\n",
      "        [1.1090],\n",
      "        [0.8351],\n",
      "        [1.2634],\n",
      "        [1.0701],\n",
      "        [1.0577],\n",
      "        [1.3566],\n",
      "        [0.7123],\n",
      "        [0.8215],\n",
      "        [1.0291],\n",
      "        [1.2345],\n",
      "        [1.4586],\n",
      "        [1.2812],\n",
      "        [0.7077],\n",
      "        [1.0880],\n",
      "        [0.8049],\n",
      "        [1.2319],\n",
      "        [1.2197],\n",
      "        [1.1261],\n",
      "        [1.3004],\n",
      "        [1.3141],\n",
      "        [1.2045],\n",
      "        [1.1816],\n",
      "        [1.0907],\n",
      "        [1.3033],\n",
      "        [1.3503],\n",
      "        [1.2527],\n",
      "        [0.9547],\n",
      "        [1.4358],\n",
      "        [1.1858],\n",
      "        [1.1612],\n",
      "        [0.8220],\n",
      "        [1.3845],\n",
      "        [1.1425],\n",
      "        [0.9688],\n",
      "        [0.9830],\n",
      "        [1.0861],\n",
      "        [1.2510],\n",
      "        [0.9184],\n",
      "        [0.9518],\n",
      "        [1.2269],\n",
      "        [1.2331],\n",
      "        [1.0974],\n",
      "        [0.8947],\n",
      "        [1.2974],\n",
      "        [1.2720],\n",
      "        [1.4656],\n",
      "        [1.2328],\n",
      "        [1.1439],\n",
      "        [1.2243],\n",
      "        [1.1205],\n",
      "        [1.4813],\n",
      "        [1.2427],\n",
      "        [1.0252],\n",
      "        [1.5134],\n",
      "        [1.2918],\n",
      "        [1.2118],\n",
      "        [0.9754],\n",
      "        [1.0603],\n",
      "        [1.2585],\n",
      "        [1.2629],\n",
      "        [0.7926],\n",
      "        [1.1745],\n",
      "        [0.6737]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.9397],\n",
      "        [1.1209],\n",
      "        [0.9104],\n",
      "        [1.1293],\n",
      "        [1.1483],\n",
      "        [1.1888],\n",
      "        [1.2788],\n",
      "        [0.9819],\n",
      "        [1.2111],\n",
      "        [1.2174],\n",
      "        [1.2138],\n",
      "        [0.9677],\n",
      "        [1.4601],\n",
      "        [1.2197],\n",
      "        [1.1634],\n",
      "        [1.1913],\n",
      "        [1.2371],\n",
      "        [1.2639],\n",
      "        [1.1839],\n",
      "        [1.1734],\n",
      "        [1.5283],\n",
      "        [1.4017],\n",
      "        [0.9037],\n",
      "        [1.2702],\n",
      "        [0.6858],\n",
      "        [1.2043],\n",
      "        [1.3466],\n",
      "        [1.4297],\n",
      "        [1.2179],\n",
      "        [1.5731],\n",
      "        [1.3425],\n",
      "        [1.1759],\n",
      "        [1.4929],\n",
      "        [1.1735],\n",
      "        [0.9200],\n",
      "        [0.7013],\n",
      "        [1.3719],\n",
      "        [0.8565],\n",
      "        [1.1942],\n",
      "        [1.0892],\n",
      "        [1.2574],\n",
      "        [1.0630],\n",
      "        [0.9830],\n",
      "        [1.0022],\n",
      "        [1.0859],\n",
      "        [1.0563],\n",
      "        [1.0933],\n",
      "        [1.4075],\n",
      "        [1.2109],\n",
      "        [1.3002],\n",
      "        [1.2009],\n",
      "        [1.2023],\n",
      "        [1.0802],\n",
      "        [1.1273],\n",
      "        [1.1050],\n",
      "        [1.2087],\n",
      "        [1.0961],\n",
      "        [0.6498],\n",
      "        [1.3312],\n",
      "        [0.7468],\n",
      "        [1.2166],\n",
      "        [1.2452],\n",
      "        [1.2089],\n",
      "        [1.5309]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.4292],\n",
      "        [1.4673],\n",
      "        [1.1669],\n",
      "        [0.8316],\n",
      "        [1.2256],\n",
      "        [1.3729],\n",
      "        [1.1817],\n",
      "        [0.7244],\n",
      "        [1.3902],\n",
      "        [1.1540],\n",
      "        [1.2158],\n",
      "        [1.0929],\n",
      "        [0.5386],\n",
      "        [1.2925],\n",
      "        [1.2405],\n",
      "        [1.0452],\n",
      "        [1.1721],\n",
      "        [1.2185],\n",
      "        [1.2165],\n",
      "        [1.1708],\n",
      "        [1.0271],\n",
      "        [1.2560],\n",
      "        [1.4783],\n",
      "        [1.1214],\n",
      "        [1.1937],\n",
      "        [1.3895],\n",
      "        [1.0257],\n",
      "        [1.4701],\n",
      "        [1.0290],\n",
      "        [1.5015],\n",
      "        [1.3231],\n",
      "        [1.1617],\n",
      "        [1.3584],\n",
      "        [1.3388],\n",
      "        [1.1535],\n",
      "        [1.1903],\n",
      "        [1.3268],\n",
      "        [0.9859],\n",
      "        [1.2513],\n",
      "        [1.4452],\n",
      "        [1.2656],\n",
      "        [1.0384],\n",
      "        [1.2289],\n",
      "        [1.2146],\n",
      "        [1.2081],\n",
      "        [1.0677],\n",
      "        [1.2117],\n",
      "        [1.4888],\n",
      "        [1.1931],\n",
      "        [1.1867],\n",
      "        [1.3135],\n",
      "        [1.0921],\n",
      "        [1.1917],\n",
      "        [1.4087],\n",
      "        [1.3743],\n",
      "        [0.9959],\n",
      "        [1.1692],\n",
      "        [0.5552],\n",
      "        [1.1028],\n",
      "        [1.0779],\n",
      "        [1.1012],\n",
      "        [1.0208],\n",
      "        [1.0524],\n",
      "        [1.0472]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.0890],\n",
      "        [1.0109],\n",
      "        [1.1946],\n",
      "        [1.1640],\n",
      "        [1.1619],\n",
      "        [1.3011],\n",
      "        [1.1261],\n",
      "        [1.1655],\n",
      "        [1.2024],\n",
      "        [1.2493],\n",
      "        [1.2081],\n",
      "        [0.6289],\n",
      "        [1.3985],\n",
      "        [1.2521],\n",
      "        [1.2402],\n",
      "        [1.2154],\n",
      "        [1.1540],\n",
      "        [1.2381],\n",
      "        [0.9994],\n",
      "        [1.3423],\n",
      "        [1.1256],\n",
      "        [1.2749],\n",
      "        [0.9157],\n",
      "        [1.1258],\n",
      "        [1.3260],\n",
      "        [1.2049],\n",
      "        [1.3945],\n",
      "        [1.1973],\n",
      "        [1.0512],\n",
      "        [0.8724],\n",
      "        [1.2975],\n",
      "        [1.3593],\n",
      "        [1.4359],\n",
      "        [1.0974],\n",
      "        [1.3815],\n",
      "        [1.1799],\n",
      "        [1.1363],\n",
      "        [1.1324],\n",
      "        [1.0981],\n",
      "        [1.4642],\n",
      "        [1.3412],\n",
      "        [1.2197],\n",
      "        [1.2171],\n",
      "        [0.7435],\n",
      "        [1.2513],\n",
      "        [1.6368],\n",
      "        [1.0792],\n",
      "        [1.1864],\n",
      "        [1.2505],\n",
      "        [1.2205],\n",
      "        [1.4553],\n",
      "        [1.2957],\n",
      "        [1.0140],\n",
      "        [1.2179],\n",
      "        [1.1480],\n",
      "        [1.3680],\n",
      "        [1.1289],\n",
      "        [0.6981],\n",
      "        [1.2111],\n",
      "        [1.1682],\n",
      "        [1.0099],\n",
      "        [1.1848],\n",
      "        [1.1248],\n",
      "        [1.1559]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[1.5236],\n",
      "        [0.8468],\n",
      "        [1.1850],\n",
      "        [1.2393],\n",
      "        [1.2544],\n",
      "        [1.1988],\n",
      "        [1.3033],\n",
      "        [1.2056],\n",
      "        [1.1534],\n",
      "        [1.2121],\n",
      "        [1.2696],\n",
      "        [1.2270],\n",
      "        [0.9939],\n",
      "        [0.9061],\n",
      "        [1.0062],\n",
      "        [0.7581],\n",
      "        [1.1236],\n",
      "        [1.0872],\n",
      "        [0.8582],\n",
      "        [1.0200],\n",
      "        [1.2372],\n",
      "        [1.3687],\n",
      "        [1.1723],\n",
      "        [1.1145],\n",
      "        [1.3944],\n",
      "        [1.2450],\n",
      "        [1.0749],\n",
      "        [1.4960],\n",
      "        [1.1233],\n",
      "        [1.2186],\n",
      "        [1.0526],\n",
      "        [1.1441],\n",
      "        [1.4488],\n",
      "        [1.2870],\n",
      "        [1.1920],\n",
      "        [1.1412],\n",
      "        [1.1012],\n",
      "        [1.1192],\n",
      "        [1.2730],\n",
      "        [1.3521],\n",
      "        [0.9945],\n",
      "        [1.2761],\n",
      "        [1.0628],\n",
      "        [1.2777],\n",
      "        [1.1702],\n",
      "        [1.2237],\n",
      "        [1.1070],\n",
      "        [1.0621],\n",
      "        [1.1280],\n",
      "        [1.2029],\n",
      "        [1.3652],\n",
      "        [1.2546],\n",
      "        [1.4318],\n",
      "        [1.3669],\n",
      "        [1.1629],\n",
      "        [1.0844],\n",
      "        [1.2930],\n",
      "        [1.2166],\n",
      "        [1.5735],\n",
      "        [1.3132],\n",
      "        [1.0439],\n",
      "        [1.1091],\n",
      "        [1.2127],\n",
      "        [1.0830]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([64, 1])\n",
      "Attention weights shape: torch.Size([64, 89, 89])\n",
      "tensor([[0.8632],\n",
      "        [1.1173],\n",
      "        [1.2320],\n",
      "        [1.1073],\n",
      "        [1.0480],\n",
      "        [1.3703],\n",
      "        [1.2260],\n",
      "        [1.1153],\n",
      "        [1.2916],\n",
      "        [1.3745],\n",
      "        [0.9542],\n",
      "        [1.1651],\n",
      "        [1.1965],\n",
      "        [1.2810],\n",
      "        [0.7279],\n",
      "        [1.2332],\n",
      "        [1.1517],\n",
      "        [1.3403],\n",
      "        [1.2385],\n",
      "        [1.0523],\n",
      "        [1.4856],\n",
      "        [1.2306],\n",
      "        [1.3545],\n",
      "        [1.2457],\n",
      "        [1.2139],\n",
      "        [1.1317],\n",
      "        [1.1750],\n",
      "        [1.1738],\n",
      "        [1.1516],\n",
      "        [1.2180],\n",
      "        [1.1950],\n",
      "        [1.0159],\n",
      "        [1.0033],\n",
      "        [1.1673],\n",
      "        [1.2047],\n",
      "        [1.0824],\n",
      "        [1.2283],\n",
      "        [1.2948],\n",
      "        [1.5275],\n",
      "        [1.4007],\n",
      "        [1.4708],\n",
      "        [1.1829],\n",
      "        [0.9896],\n",
      "        [1.1527],\n",
      "        [1.1188],\n",
      "        [1.2112],\n",
      "        [1.1017],\n",
      "        [1.0636],\n",
      "        [1.4629],\n",
      "        [1.0768],\n",
      "        [1.1095],\n",
      "        [1.2040],\n",
      "        [1.0069],\n",
      "        [1.2758],\n",
      "        [1.1643],\n",
      "        [0.8676],\n",
      "        [1.3892],\n",
      "        [1.1791],\n",
      "        [1.4642],\n",
      "        [1.3461],\n",
      "        [1.3602],\n",
      "        [1.2403],\n",
      "        [1.2212],\n",
      "        [1.1782]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([35, 1])\n",
      "Attention weights shape: torch.Size([35, 89, 89])\n",
      "tensor([[1.1809],\n",
      "        [1.2381],\n",
      "        [1.2622],\n",
      "        [1.1644],\n",
      "        [1.4365],\n",
      "        [1.3021],\n",
      "        [1.0595],\n",
      "        [1.0736],\n",
      "        [1.1525],\n",
      "        [1.4164],\n",
      "        [1.4051],\n",
      "        [1.2886],\n",
      "        [1.2129],\n",
      "        [1.3575],\n",
      "        [1.2995],\n",
      "        [1.2764],\n",
      "        [1.2389],\n",
      "        [1.3400],\n",
      "        [1.4934],\n",
      "        [0.8989],\n",
      "        [0.8618],\n",
      "        [1.2354],\n",
      "        [1.1732],\n",
      "        [1.0874],\n",
      "        [0.4978],\n",
      "        [1.1607],\n",
      "        [1.2407],\n",
      "        [1.2898],\n",
      "        [1.0495],\n",
      "        [0.9201],\n",
      "        [1.2241],\n",
      "        [0.8485],\n",
      "        [1.1358],\n",
      "        [1.4591],\n",
      "        [1.0281]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#GPU run\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "# 1. Wrap data in a Dataset\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(obs_tr).float(),\n",
    "    torch.tensor(known_tr).float(),\n",
    "    torch.tensor(static_tr).float()\n",
    ")\n",
    "\n",
    "# 2. Create a DataLoader for batching\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "print(f\"Starting inference on {len(dataset)} samples...\")\n",
    "\n",
    "for batch_obs, batch_known, batch_static in tqdm(train_loader, desc=\"Processing Batches\"):\n",
    "\n",
    "    batch_obs = batch_obs.to(device)\n",
    "    batch_known = batch_known.to(device)\n",
    "    batch_static = batch_static.to(device)\n",
    "    \n",
    "    output, _ = model(batch_obs, batch_known, batch_static)\n",
    "    \n",
    "    all_predictions.append(output.cpu())\n",
    "\n",
    "# Combine all batches into one large tensor\n",
    "final_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "print(\" Dataset processed successfully!\")\n",
    "print(f\"Final Prediction Shape: {final_predictions.shape}\")\n",
    "print(\"Sample Output (first 5 rows):\")\n",
    "print(final_predictions[:5])\n",
    "\n",
    "# --- EXPORTING THE MODEL FOR BACKEND ---\n",
    "# We save the 'state_dict' (the learned weights)\n",
    "model_path = \"mini_tft_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"\\n Model exported successfully to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
