{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d87cf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "        self.gate = nn.Linear(d_out, d_out)\n",
    "        self.skip = nn.Linear(d_in, d_out) if d_in != d_out else nn.Identity()\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.elu(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        g = torch.sigmoid(self.gate(h))\n",
    "        return self.norm(g * h + (1 - g) * self.skip(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticEncoder(nn.Module):\n",
    "    def __init__(self, d_static, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_static, d_model, d_model)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.grn(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, num_vars, d_model):\n",
    "        super().__init__()\n",
    "        self.var_grns = nn.ModuleList([\n",
    "            GRN(1, d_model, d_model) for _ in range(num_vars)\n",
    "        ])\n",
    "        self.weight_grn = GRN(num_vars, d_model, num_vars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, num_vars]\n",
    "        var_embeds = []\n",
    "        for i, grn in enumerate(self.var_grns):\n",
    "            var_embeds.append(grn(x[..., i:i+1]))\n",
    "        var_embeds = torch.stack(var_embeds, dim=-2)  # [B,T,num_vars,d]\n",
    "\n",
    "        weights = self.weight_grn(x).softmax(dim=-1)  # [B,T,num_vars]\n",
    "        fused = (weights.unsqueeze(-1) * var_embeds).sum(dim=-2)\n",
    "        return fused, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEnrichment(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_model * 2, d_model, d_model)\n",
    "\n",
    "    def forward(self, temporal, context):\n",
    "        context = context.unsqueeze(1).expand_as(temporal)\n",
    "        return self.grn(torch.cat([temporal, context], dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79321bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.grn = GRN(d_model, d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "        attn_out, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        out = self.grn(attn_out + x)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTFT(nn.Module):\n",
    "    def __init__(self, n_obs, n_known, d_static, d_model=32):\n",
    "        super().__init__()\n",
    "        self.static_enc = StaticEncoder(d_static, d_model)\n",
    "\n",
    "        self.obs_vsn = VariableSelectionNetwork(n_obs, d_model)\n",
    "        self.known_vsn = VariableSelectionNetwork(n_known, d_model)\n",
    "\n",
    "        self.enrich = ContextEnrichment(d_model)\n",
    "        self.attn = TemporalAttention(d_model, num_heads=4)\n",
    "        \n",
    "        self.post_attn_grn = GRN(d_model, d_model, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.head = PredictionHead(d_model)\n",
    "\n",
    "    def forward(self, obs, known, static):\n",
    "        # Preparation for 3 types of inputs\n",
    "        s = self.static_enc(static)\n",
    "        obs_fused,_ = self.obs_vsn(obs)\n",
    "        known_fused,_ = self.known_vsn(known)\n",
    "        # Locality enhanchement\n",
    "        x = obs_fused + known_fused\n",
    "        # Temporal processing\n",
    "        x = self.enrich(x, s)\n",
    "        # Temporal attention\n",
    "        attn_out,attn_weights = self.attn(x)\n",
    "        x = self.layer_norm(attn_out + x)\n",
    "        x = self.layer_norm(self.post_attn_grn(x) + x)\n",
    "        return self.head(x), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ef317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(\"../../data/features/BTC_features.parquet\")\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head(1))\n",
    "print(\"---------------------------------------\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\"symbol\", \"source\"]\n",
    "observed_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"ema_34\", \"ema_89\", \"ema_200\",\n",
    "    \"rsi_14\", \"macd\", \"log_return\", \"vol_20\"\n",
    "] + [c for c in df.columns if \"lag_\" in c]\n",
    "known_cols = [\n",
    "    \"building_permits\", \"consumer_confidence\", \"cpi\",\n",
    "    \"fed_funds_rate\", \"gdp\", \"industrial_production\",\n",
    "    \"money_supply_m1\", \"money_supply_m2\",\n",
    "    \"nonfarm_payrolls\", \"pce_inflation\",\n",
    "    \"ppi\", \"retail_sales\", \"trade_balance\",\n",
    "    \"unemployment_rate\"\n",
    "] + [c for c in df.columns if c.startswith(\"fed_emb_\")]\n",
    "target_col = \"next_close\"\n",
    "\n",
    "# train val test splitting\n",
    "n = len(df)\n",
    "train_end = int(0.70 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val   = df.iloc[train_end:val_end]\n",
    "df_test  = df.iloc[val_end:]\n",
    "\n",
    "\n",
    "X_train = df_train[observed_cols + known_cols]\n",
    "X_val   = df_val[observed_cols + known_cols]\n",
    "X_test  = df_test[observed_cols + known_cols]\n",
    "\n",
    "y_train = df_train[target_col].values\n",
    "y_val   = df_val[target_col].values\n",
    "y_test  = df_test[target_col].values\n",
    "# Sliding window\n",
    "LOOKBACK = 89\n",
    "\n",
    "def build_windows(X, y, lookback):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        X_out.append(X[i-lookback:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "Xtr, ytr = build_windows(X_train, y_train, LOOKBACK)\n",
    "Xva, yva = build_windows(X_val, y_val, LOOKBACK)\n",
    "Xte, yte = build_windows(X_test, y_test, LOOKBACK)\n",
    "\n",
    "# split input feature tensors\n",
    "n_obs   = len(observed_cols)\n",
    "n_known = len(known_cols)\n",
    "#Tensor\n",
    "obs_tr   = Xtr[:, :, :n_obs]\n",
    "known_tr = Xtr[:, :, n_obs:]\n",
    "\n",
    "static_tr = np.zeros((len(Xtr), 2))  # placeholder encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ad131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Total parameters (including those frozen/not being trained)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Trainable parameters only\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage:\n",
    "model = MiniTFT(n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd760d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "pred = model(\n",
    "    torch.tensor(obs_tr).float().to(device),\n",
    "    torch.tensor(known_tr).float().to(device),\n",
    "    torch.tensor(static_tr).float().to(device)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
