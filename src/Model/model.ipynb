{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d87cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffa8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "        self.gate = nn.Linear(d_out, d_out)\n",
    "        self.skip = nn.Linear(d_in, d_out) if d_in != d_out else nn.Identity()\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.elu(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        g = torch.sigmoid(self.gate(h))\n",
    "        return self.norm(g * h + (1 - g) * self.skip(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticEncoder(nn.Module):\n",
    "    def __init__(self, d_static, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_static, d_model, d_model)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.grn(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0e2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, num_vars, d_model):\n",
    "        super().__init__()\n",
    "        self.value_proj = nn.Linear(num_vars, d_model)\n",
    "        self.weight_proj = nn.Linear(num_vars, num_vars)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, num_vars]\n",
    "        values = self.value_proj(x)               # [B, T, d_model]\n",
    "        weights = self.softmax(self.weight_proj(x))  # [B, T, num_vars]\n",
    "        fused = values\n",
    "        return fused, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c454ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEnrichment(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.grn = GRN(d_model * 2, d_model, d_model)\n",
    "\n",
    "    def forward(self, temporal, context):\n",
    "        context = context.unsqueeze(1).expand_as(temporal)\n",
    "        return self.grn(torch.cat([temporal, context], dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79321bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.grn = GRN(d_model, d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "        attn_out, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        out = self.grn(attn_out + x)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1977cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a43b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTFT(nn.Module):\n",
    "    def __init__(self, n_obs, n_known, d_static, d_model=32):\n",
    "        super().__init__()\n",
    "        self.static_enc = StaticEncoder(d_static, d_model)\n",
    "\n",
    "        self.obs_vsn = VariableSelectionNetwork(n_obs, d_model)\n",
    "        self.known_vsn = VariableSelectionNetwork(n_known, d_model)\n",
    "\n",
    "        self.enrich = ContextEnrichment(d_model)\n",
    "        self.attn = TemporalAttention(d_model, num_heads=4)\n",
    "        \n",
    "        self.post_attn_grn = GRN(d_model, d_model, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.head = PredictionHead(d_model)\n",
    "\n",
    "    def forward(self, obs, known, static):\n",
    "        # Preparation for 3 types of inputs\n",
    "        s = self.static_enc(static)\n",
    "        obs_fused,_ = self.obs_vsn(obs)\n",
    "        known_fused,_ = self.known_vsn(known)\n",
    "        # Locality enhanchement\n",
    "        x = obs_fused + known_fused\n",
    "        # Temporal processing\n",
    "        x = self.enrich(x, s)\n",
    "        # Temporal attention\n",
    "        attn_out,attn_weights = self.attn(x)\n",
    "        x = self.layer_norm(attn_out + x)\n",
    "        x = self.layer_norm(self.post_attn_grn(x) + x)\n",
    "        return self.head(x), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62ef317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 819)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 5481 entries, 2010-12-09 to 2025-12-10\n",
      "Columns: 819 entries, open to ret_lag_144\n",
      "dtypes: float64(817), object(2)\n",
      "memory usage: 34.3+ MB\n",
      "None\n",
      "                open      high      low     close    volume symbol  \\\n",
      "date                                                                 \n",
      "2010-12-09 -0.674481 -0.676188 -0.67272 -0.674708 -0.640733    BTC   \n",
      "\n",
      "                   source  building_permits  consumer_confidence       cpi  \\\n",
      "date                                                                         \n",
      "2010-12-09  alpha_vantage         -2.064399            -0.387082 -1.290801   \n",
      "\n",
      "            ...  close_lag_21  ret_lag_21  close_lag_34  ret_lag_34  \\\n",
      "date        ...                                                       \n",
      "2010-12-09  ...     -0.669729    3.274749     -0.667228    2.574162   \n",
      "\n",
      "            close_lag_55  ret_lag_55  close_lag_89  ret_lag_89  close_lag_144  \\\n",
      "date                                                                            \n",
      "2010-12-09     -0.664618    0.563059     -0.662986    0.586438      -0.662793   \n",
      "\n",
      "            ret_lag_144  \n",
      "date                     \n",
      "2010-12-09    11.081548  \n",
      "\n",
      "[1 rows x 819 columns]\n",
      "---------------------------------------\n",
      "['open', 'high', 'low', 'close', 'volume', 'symbol', 'source', 'building_permits', 'consumer_confidence', 'cpi', 'fed_funds_rate', 'gdp', 'industrial_production', 'money_supply_m1', 'money_supply_m2', 'nonfarm_payrolls', 'pce_inflation', 'ppi', 'retail_sales', 'trade_balance', 'unemployment_rate', 'fed_emb_0', 'fed_emb_1', 'fed_emb_2', 'fed_emb_3', 'fed_emb_4', 'fed_emb_5', 'fed_emb_6', 'fed_emb_7', 'fed_emb_8', 'fed_emb_9', 'fed_emb_10', 'fed_emb_11', 'fed_emb_12', 'fed_emb_13', 'fed_emb_14', 'fed_emb_15', 'fed_emb_16', 'fed_emb_17', 'fed_emb_18', 'fed_emb_19', 'fed_emb_20', 'fed_emb_21', 'fed_emb_22', 'fed_emb_23', 'fed_emb_24', 'fed_emb_25', 'fed_emb_26', 'fed_emb_27', 'fed_emb_28', 'fed_emb_29', 'fed_emb_30', 'fed_emb_31', 'fed_emb_32', 'fed_emb_33', 'fed_emb_34', 'fed_emb_35', 'fed_emb_36', 'fed_emb_37', 'fed_emb_38', 'fed_emb_39', 'fed_emb_40', 'fed_emb_41', 'fed_emb_42', 'fed_emb_43', 'fed_emb_44', 'fed_emb_45', 'fed_emb_46', 'fed_emb_47', 'fed_emb_48', 'fed_emb_49', 'fed_emb_50', 'fed_emb_51', 'fed_emb_52', 'fed_emb_53', 'fed_emb_54', 'fed_emb_55', 'fed_emb_56', 'fed_emb_57', 'fed_emb_58', 'fed_emb_59', 'fed_emb_60', 'fed_emb_61', 'fed_emb_62', 'fed_emb_63', 'fed_emb_64', 'fed_emb_65', 'fed_emb_66', 'fed_emb_67', 'fed_emb_68', 'fed_emb_69', 'fed_emb_70', 'fed_emb_71', 'fed_emb_72', 'fed_emb_73', 'fed_emb_74', 'fed_emb_75', 'fed_emb_76', 'fed_emb_77', 'fed_emb_78', 'fed_emb_79', 'fed_emb_80', 'fed_emb_81', 'fed_emb_82', 'fed_emb_83', 'fed_emb_84', 'fed_emb_85', 'fed_emb_86', 'fed_emb_87', 'fed_emb_88', 'fed_emb_89', 'fed_emb_90', 'fed_emb_91', 'fed_emb_92', 'fed_emb_93', 'fed_emb_94', 'fed_emb_95', 'fed_emb_96', 'fed_emb_97', 'fed_emb_98', 'fed_emb_99', 'fed_emb_100', 'fed_emb_101', 'fed_emb_102', 'fed_emb_103', 'fed_emb_104', 'fed_emb_105', 'fed_emb_106', 'fed_emb_107', 'fed_emb_108', 'fed_emb_109', 'fed_emb_110', 'fed_emb_111', 'fed_emb_112', 'fed_emb_113', 'fed_emb_114', 'fed_emb_115', 'fed_emb_116', 'fed_emb_117', 'fed_emb_118', 'fed_emb_119', 'fed_emb_120', 'fed_emb_121', 'fed_emb_122', 'fed_emb_123', 'fed_emb_124', 'fed_emb_125', 'fed_emb_126', 'fed_emb_127', 'fed_emb_128', 'fed_emb_129', 'fed_emb_130', 'fed_emb_131', 'fed_emb_132', 'fed_emb_133', 'fed_emb_134', 'fed_emb_135', 'fed_emb_136', 'fed_emb_137', 'fed_emb_138', 'fed_emb_139', 'fed_emb_140', 'fed_emb_141', 'fed_emb_142', 'fed_emb_143', 'fed_emb_144', 'fed_emb_145', 'fed_emb_146', 'fed_emb_147', 'fed_emb_148', 'fed_emb_149', 'fed_emb_150', 'fed_emb_151', 'fed_emb_152', 'fed_emb_153', 'fed_emb_154', 'fed_emb_155', 'fed_emb_156', 'fed_emb_157', 'fed_emb_158', 'fed_emb_159', 'fed_emb_160', 'fed_emb_161', 'fed_emb_162', 'fed_emb_163', 'fed_emb_164', 'fed_emb_165', 'fed_emb_166', 'fed_emb_167', 'fed_emb_168', 'fed_emb_169', 'fed_emb_170', 'fed_emb_171', 'fed_emb_172', 'fed_emb_173', 'fed_emb_174', 'fed_emb_175', 'fed_emb_176', 'fed_emb_177', 'fed_emb_178', 'fed_emb_179', 'fed_emb_180', 'fed_emb_181', 'fed_emb_182', 'fed_emb_183', 'fed_emb_184', 'fed_emb_185', 'fed_emb_186', 'fed_emb_187', 'fed_emb_188', 'fed_emb_189', 'fed_emb_190', 'fed_emb_191', 'fed_emb_192', 'fed_emb_193', 'fed_emb_194', 'fed_emb_195', 'fed_emb_196', 'fed_emb_197', 'fed_emb_198', 'fed_emb_199', 'fed_emb_200', 'fed_emb_201', 'fed_emb_202', 'fed_emb_203', 'fed_emb_204', 'fed_emb_205', 'fed_emb_206', 'fed_emb_207', 'fed_emb_208', 'fed_emb_209', 'fed_emb_210', 'fed_emb_211', 'fed_emb_212', 'fed_emb_213', 'fed_emb_214', 'fed_emb_215', 'fed_emb_216', 'fed_emb_217', 'fed_emb_218', 'fed_emb_219', 'fed_emb_220', 'fed_emb_221', 'fed_emb_222', 'fed_emb_223', 'fed_emb_224', 'fed_emb_225', 'fed_emb_226', 'fed_emb_227', 'fed_emb_228', 'fed_emb_229', 'fed_emb_230', 'fed_emb_231', 'fed_emb_232', 'fed_emb_233', 'fed_emb_234', 'fed_emb_235', 'fed_emb_236', 'fed_emb_237', 'fed_emb_238', 'fed_emb_239', 'fed_emb_240', 'fed_emb_241', 'fed_emb_242', 'fed_emb_243', 'fed_emb_244', 'fed_emb_245', 'fed_emb_246', 'fed_emb_247', 'fed_emb_248', 'fed_emb_249', 'fed_emb_250', 'fed_emb_251', 'fed_emb_252', 'fed_emb_253', 'fed_emb_254', 'fed_emb_255', 'fed_emb_256', 'fed_emb_257', 'fed_emb_258', 'fed_emb_259', 'fed_emb_260', 'fed_emb_261', 'fed_emb_262', 'fed_emb_263', 'fed_emb_264', 'fed_emb_265', 'fed_emb_266', 'fed_emb_267', 'fed_emb_268', 'fed_emb_269', 'fed_emb_270', 'fed_emb_271', 'fed_emb_272', 'fed_emb_273', 'fed_emb_274', 'fed_emb_275', 'fed_emb_276', 'fed_emb_277', 'fed_emb_278', 'fed_emb_279', 'fed_emb_280', 'fed_emb_281', 'fed_emb_282', 'fed_emb_283', 'fed_emb_284', 'fed_emb_285', 'fed_emb_286', 'fed_emb_287', 'fed_emb_288', 'fed_emb_289', 'fed_emb_290', 'fed_emb_291', 'fed_emb_292', 'fed_emb_293', 'fed_emb_294', 'fed_emb_295', 'fed_emb_296', 'fed_emb_297', 'fed_emb_298', 'fed_emb_299', 'fed_emb_300', 'fed_emb_301', 'fed_emb_302', 'fed_emb_303', 'fed_emb_304', 'fed_emb_305', 'fed_emb_306', 'fed_emb_307', 'fed_emb_308', 'fed_emb_309', 'fed_emb_310', 'fed_emb_311', 'fed_emb_312', 'fed_emb_313', 'fed_emb_314', 'fed_emb_315', 'fed_emb_316', 'fed_emb_317', 'fed_emb_318', 'fed_emb_319', 'fed_emb_320', 'fed_emb_321', 'fed_emb_322', 'fed_emb_323', 'fed_emb_324', 'fed_emb_325', 'fed_emb_326', 'fed_emb_327', 'fed_emb_328', 'fed_emb_329', 'fed_emb_330', 'fed_emb_331', 'fed_emb_332', 'fed_emb_333', 'fed_emb_334', 'fed_emb_335', 'fed_emb_336', 'fed_emb_337', 'fed_emb_338', 'fed_emb_339', 'fed_emb_340', 'fed_emb_341', 'fed_emb_342', 'fed_emb_343', 'fed_emb_344', 'fed_emb_345', 'fed_emb_346', 'fed_emb_347', 'fed_emb_348', 'fed_emb_349', 'fed_emb_350', 'fed_emb_351', 'fed_emb_352', 'fed_emb_353', 'fed_emb_354', 'fed_emb_355', 'fed_emb_356', 'fed_emb_357', 'fed_emb_358', 'fed_emb_359', 'fed_emb_360', 'fed_emb_361', 'fed_emb_362', 'fed_emb_363', 'fed_emb_364', 'fed_emb_365', 'fed_emb_366', 'fed_emb_367', 'fed_emb_368', 'fed_emb_369', 'fed_emb_370', 'fed_emb_371', 'fed_emb_372', 'fed_emb_373', 'fed_emb_374', 'fed_emb_375', 'fed_emb_376', 'fed_emb_377', 'fed_emb_378', 'fed_emb_379', 'fed_emb_380', 'fed_emb_381', 'fed_emb_382', 'fed_emb_383', 'fed_emb_384', 'fed_emb_385', 'fed_emb_386', 'fed_emb_387', 'fed_emb_388', 'fed_emb_389', 'fed_emb_390', 'fed_emb_391', 'fed_emb_392', 'fed_emb_393', 'fed_emb_394', 'fed_emb_395', 'fed_emb_396', 'fed_emb_397', 'fed_emb_398', 'fed_emb_399', 'fed_emb_400', 'fed_emb_401', 'fed_emb_402', 'fed_emb_403', 'fed_emb_404', 'fed_emb_405', 'fed_emb_406', 'fed_emb_407', 'fed_emb_408', 'fed_emb_409', 'fed_emb_410', 'fed_emb_411', 'fed_emb_412', 'fed_emb_413', 'fed_emb_414', 'fed_emb_415', 'fed_emb_416', 'fed_emb_417', 'fed_emb_418', 'fed_emb_419', 'fed_emb_420', 'fed_emb_421', 'fed_emb_422', 'fed_emb_423', 'fed_emb_424', 'fed_emb_425', 'fed_emb_426', 'fed_emb_427', 'fed_emb_428', 'fed_emb_429', 'fed_emb_430', 'fed_emb_431', 'fed_emb_432', 'fed_emb_433', 'fed_emb_434', 'fed_emb_435', 'fed_emb_436', 'fed_emb_437', 'fed_emb_438', 'fed_emb_439', 'fed_emb_440', 'fed_emb_441', 'fed_emb_442', 'fed_emb_443', 'fed_emb_444', 'fed_emb_445', 'fed_emb_446', 'fed_emb_447', 'fed_emb_448', 'fed_emb_449', 'fed_emb_450', 'fed_emb_451', 'fed_emb_452', 'fed_emb_453', 'fed_emb_454', 'fed_emb_455', 'fed_emb_456', 'fed_emb_457', 'fed_emb_458', 'fed_emb_459', 'fed_emb_460', 'fed_emb_461', 'fed_emb_462', 'fed_emb_463', 'fed_emb_464', 'fed_emb_465', 'fed_emb_466', 'fed_emb_467', 'fed_emb_468', 'fed_emb_469', 'fed_emb_470', 'fed_emb_471', 'fed_emb_472', 'fed_emb_473', 'fed_emb_474', 'fed_emb_475', 'fed_emb_476', 'fed_emb_477', 'fed_emb_478', 'fed_emb_479', 'fed_emb_480', 'fed_emb_481', 'fed_emb_482', 'fed_emb_483', 'fed_emb_484', 'fed_emb_485', 'fed_emb_486', 'fed_emb_487', 'fed_emb_488', 'fed_emb_489', 'fed_emb_490', 'fed_emb_491', 'fed_emb_492', 'fed_emb_493', 'fed_emb_494', 'fed_emb_495', 'fed_emb_496', 'fed_emb_497', 'fed_emb_498', 'fed_emb_499', 'fed_emb_500', 'fed_emb_501', 'fed_emb_502', 'fed_emb_503', 'fed_emb_504', 'fed_emb_505', 'fed_emb_506', 'fed_emb_507', 'fed_emb_508', 'fed_emb_509', 'fed_emb_510', 'fed_emb_511', 'fed_emb_512', 'fed_emb_513', 'fed_emb_514', 'fed_emb_515', 'fed_emb_516', 'fed_emb_517', 'fed_emb_518', 'fed_emb_519', 'fed_emb_520', 'fed_emb_521', 'fed_emb_522', 'fed_emb_523', 'fed_emb_524', 'fed_emb_525', 'fed_emb_526', 'fed_emb_527', 'fed_emb_528', 'fed_emb_529', 'fed_emb_530', 'fed_emb_531', 'fed_emb_532', 'fed_emb_533', 'fed_emb_534', 'fed_emb_535', 'fed_emb_536', 'fed_emb_537', 'fed_emb_538', 'fed_emb_539', 'fed_emb_540', 'fed_emb_541', 'fed_emb_542', 'fed_emb_543', 'fed_emb_544', 'fed_emb_545', 'fed_emb_546', 'fed_emb_547', 'fed_emb_548', 'fed_emb_549', 'fed_emb_550', 'fed_emb_551', 'fed_emb_552', 'fed_emb_553', 'fed_emb_554', 'fed_emb_555', 'fed_emb_556', 'fed_emb_557', 'fed_emb_558', 'fed_emb_559', 'fed_emb_560', 'fed_emb_561', 'fed_emb_562', 'fed_emb_563', 'fed_emb_564', 'fed_emb_565', 'fed_emb_566', 'fed_emb_567', 'fed_emb_568', 'fed_emb_569', 'fed_emb_570', 'fed_emb_571', 'fed_emb_572', 'fed_emb_573', 'fed_emb_574', 'fed_emb_575', 'fed_emb_576', 'fed_emb_577', 'fed_emb_578', 'fed_emb_579', 'fed_emb_580', 'fed_emb_581', 'fed_emb_582', 'fed_emb_583', 'fed_emb_584', 'fed_emb_585', 'fed_emb_586', 'fed_emb_587', 'fed_emb_588', 'fed_emb_589', 'fed_emb_590', 'fed_emb_591', 'fed_emb_592', 'fed_emb_593', 'fed_emb_594', 'fed_emb_595', 'fed_emb_596', 'fed_emb_597', 'fed_emb_598', 'fed_emb_599', 'fed_emb_600', 'fed_emb_601', 'fed_emb_602', 'fed_emb_603', 'fed_emb_604', 'fed_emb_605', 'fed_emb_606', 'fed_emb_607', 'fed_emb_608', 'fed_emb_609', 'fed_emb_610', 'fed_emb_611', 'fed_emb_612', 'fed_emb_613', 'fed_emb_614', 'fed_emb_615', 'fed_emb_616', 'fed_emb_617', 'fed_emb_618', 'fed_emb_619', 'fed_emb_620', 'fed_emb_621', 'fed_emb_622', 'fed_emb_623', 'fed_emb_624', 'fed_emb_625', 'fed_emb_626', 'fed_emb_627', 'fed_emb_628', 'fed_emb_629', 'fed_emb_630', 'fed_emb_631', 'fed_emb_632', 'fed_emb_633', 'fed_emb_634', 'fed_emb_635', 'fed_emb_636', 'fed_emb_637', 'fed_emb_638', 'fed_emb_639', 'fed_emb_640', 'fed_emb_641', 'fed_emb_642', 'fed_emb_643', 'fed_emb_644', 'fed_emb_645', 'fed_emb_646', 'fed_emb_647', 'fed_emb_648', 'fed_emb_649', 'fed_emb_650', 'fed_emb_651', 'fed_emb_652', 'fed_emb_653', 'fed_emb_654', 'fed_emb_655', 'fed_emb_656', 'fed_emb_657', 'fed_emb_658', 'fed_emb_659', 'fed_emb_660', 'fed_emb_661', 'fed_emb_662', 'fed_emb_663', 'fed_emb_664', 'fed_emb_665', 'fed_emb_666', 'fed_emb_667', 'fed_emb_668', 'fed_emb_669', 'fed_emb_670', 'fed_emb_671', 'fed_emb_672', 'fed_emb_673', 'fed_emb_674', 'fed_emb_675', 'fed_emb_676', 'fed_emb_677', 'fed_emb_678', 'fed_emb_679', 'fed_emb_680', 'fed_emb_681', 'fed_emb_682', 'fed_emb_683', 'fed_emb_684', 'fed_emb_685', 'fed_emb_686', 'fed_emb_687', 'fed_emb_688', 'fed_emb_689', 'fed_emb_690', 'fed_emb_691', 'fed_emb_692', 'fed_emb_693', 'fed_emb_694', 'fed_emb_695', 'fed_emb_696', 'fed_emb_697', 'fed_emb_698', 'fed_emb_699', 'fed_emb_700', 'fed_emb_701', 'fed_emb_702', 'fed_emb_703', 'fed_emb_704', 'fed_emb_705', 'fed_emb_706', 'fed_emb_707', 'fed_emb_708', 'fed_emb_709', 'fed_emb_710', 'fed_emb_711', 'fed_emb_712', 'fed_emb_713', 'fed_emb_714', 'fed_emb_715', 'fed_emb_716', 'fed_emb_717', 'fed_emb_718', 'fed_emb_719', 'fed_emb_720', 'fed_emb_721', 'fed_emb_722', 'fed_emb_723', 'fed_emb_724', 'fed_emb_725', 'fed_emb_726', 'fed_emb_727', 'fed_emb_728', 'fed_emb_729', 'fed_emb_730', 'fed_emb_731', 'fed_emb_732', 'fed_emb_733', 'fed_emb_734', 'fed_emb_735', 'fed_emb_736', 'fed_emb_737', 'fed_emb_738', 'fed_emb_739', 'fed_emb_740', 'fed_emb_741', 'fed_emb_742', 'fed_emb_743', 'fed_emb_744', 'fed_emb_745', 'fed_emb_746', 'fed_emb_747', 'fed_emb_748', 'fed_emb_749', 'fed_emb_750', 'fed_emb_751', 'fed_emb_752', 'fed_emb_753', 'fed_emb_754', 'fed_emb_755', 'fed_emb_756', 'fed_emb_757', 'fed_emb_758', 'fed_emb_759', 'fed_emb_760', 'fed_emb_761', 'fed_emb_762', 'fed_emb_763', 'fed_emb_764', 'fed_emb_765', 'fed_emb_766', 'fed_emb_767', 'next_close', 'ema_34', 'ema_89', 'ema_200', 'rsi_14', 'macd', 'log_return', 'vol_20', 'close_lag_1', 'ret_lag_1', 'close_lag_2', 'ret_lag_2', 'close_lag_3', 'ret_lag_3', 'close_lag_5', 'ret_lag_5', 'close_lag_8', 'ret_lag_8', 'close_lag_13', 'ret_lag_13', 'close_lag_21', 'ret_lag_21', 'close_lag_34', 'ret_lag_34', 'close_lag_55', 'ret_lag_55', 'close_lag_89', 'ret_lag_89', 'close_lag_144', 'ret_lag_144']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_parquet(\"../../data/features/BTC_features.parquet\")\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head(1))\n",
    "print(\"---------------------------------------\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81e13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\"symbol\", \"source\"]\n",
    "observed_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"ema_34\", \"ema_89\", \"ema_200\",\n",
    "    \"rsi_14\", \"macd\", \"log_return\", \"vol_20\"\n",
    "] + [c for c in df.columns if \"lag_\" in c]\n",
    "known_cols = [\n",
    "    \"building_permits\", \"consumer_confidence\", \"cpi\",\n",
    "    \"fed_funds_rate\", \"gdp\", \"industrial_production\",\n",
    "    \"money_supply_m1\", \"money_supply_m2\",\n",
    "    \"nonfarm_payrolls\", \"pce_inflation\",\n",
    "    \"ppi\", \"retail_sales\", \"trade_balance\",\n",
    "    \"unemployment_rate\"\n",
    "] + [c for c in df.columns if c.startswith(\"fed_emb_\")]\n",
    "target_col = \"next_close\"\n",
    "\n",
    "# train val test splitting\n",
    "n = len(df)\n",
    "train_end = int(0.70 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val   = df.iloc[train_end:val_end]\n",
    "df_test  = df.iloc[val_end:]\n",
    "\n",
    "\n",
    "X_train = df_train[observed_cols + known_cols]\n",
    "X_val   = df_val[observed_cols + known_cols]\n",
    "X_test  = df_test[observed_cols + known_cols]\n",
    "\n",
    "y_train = df_train[target_col].values\n",
    "y_val   = df_val[target_col].values\n",
    "y_test  = df_test[target_col].values\n",
    "# Sliding window\n",
    "LOOKBACK = 89\n",
    "\n",
    "def build_windows(X, y, lookback):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        X_out.append(X[i-lookback:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "Xtr, ytr = build_windows(X_train, y_train, LOOKBACK)\n",
    "Xva, yva = build_windows(X_val, y_val, LOOKBACK)\n",
    "Xte, yte = build_windows(X_test, y_test, LOOKBACK)\n",
    "\n",
    "# split input feature tensors\n",
    "n_obs   = len(observed_cols)\n",
    "n_known = len(known_cols)\n",
    "#Tensor\n",
    "obs_tr   = Xtr[:, :, :n_obs]\n",
    "known_tr = Xtr[:, :, n_obs:]\n",
    "\n",
    "static_tr = np.zeros((len(Xtr), 2))  # placeholder encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874ad131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 659,161\n",
      "Trainable Parameters: 659,161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "659161"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    # Total parameters (including those frozen/not being trained)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Trainable parameters only\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage:\n",
    "model = MiniTFT(n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedd2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd760d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPu run\n",
    "device = \"cpu\"\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "pred = model(\n",
    "    torch.tensor(obs_tr).float().to(device),\n",
    "    torch.tensor(known_tr).float().to(device),\n",
    "    torch.tensor(static_tr).float().to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d5ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: torch.Size([32, 1])\n",
      "Attention weights shape: torch.Size([32, 89, 89])\n"
     ]
    }
   ],
   "source": [
    "# Testing only \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Try only 32 samples instead of the whole dataset\n",
    "batch_size = 32\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad(): # Use this for testing\n",
    "    pred, weights = model(\n",
    "        torch.tensor(obs_tr[:batch_size]).float().to(device),\n",
    "        torch.tensor(known_tr[:batch_size]).float().to(device),\n",
    "        torch.tensor(static_tr[:batch_size]).float().to(device)\n",
    "    )\n",
    "\n",
    "print(\"Prediction shape:\", pred.shape)\n",
    "print(\"Attention weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36372130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_3684\\3449828674.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on 3747 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/469 [00:00<?, ?it/s]C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_3684\\3449828674.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Processing Batches: 100%|██████████| 469/469 [00:11<00:00, 42.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 128499375.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 66.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 129387471.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 66.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 127096782.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 65.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 125750588.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 65.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 124194728.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 65.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 122495367.6297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 67.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 120652151.2581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 66.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 118474632.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 66.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 116329860.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:07<00:00, 65.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 114048092.1588\n",
      " Model exported successfully to: mini_tft_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPU run\n",
    "device = \"cuda\"\n",
    "\n",
    "model = MiniTFT(\n",
    "    n_obs=n_obs,\n",
    "    n_known=n_known,\n",
    "    d_static=2,\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "# 1. Wrap data in a Dataset\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(obs_tr).float(),\n",
    "    torch.tensor(known_tr).float(),\n",
    "    torch.tensor(static_tr).float(),\n",
    "    torch.tensor(ytr).float()\n",
    ")\n",
    "\n",
    "# 2. Create a DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True,persistent_workers=True, num_workers=4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(f\"Starting inference on {len(train_dataset)} samples...\")\n",
    "for epoch in range(10):  \n",
    "    total_losses = 0.0\n",
    "    for batch_obs, batch_known, batch_static, target in tqdm(train_loader, desc=\"Processing Batches\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast():\n",
    "            batch_obs = batch_obs.to(device)\n",
    "            batch_known = batch_known.to(device)\n",
    "            batch_static = batch_static.to(device)\n",
    "            target =target.to(device).unsqueeze(-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            output, _ = model(batch_obs, batch_known, batch_static)\n",
    "            loss = criterion(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_losses += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_losses/len(train_loader):.4f}\")\n",
    "\n",
    "model_path = \"mini_tft_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\" Model exported successfully to: {model_path}\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
