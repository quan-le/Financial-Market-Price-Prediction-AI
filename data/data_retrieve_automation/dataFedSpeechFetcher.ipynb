{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcf649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d2cd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\\fed_scraper\n",
      "C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\n",
      "Performing incremental scrape...\n",
      "Documents dir exists: C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\\data\\documents_by_type True\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "RAW_CSV = \"../../data/raw_speeches.csv\"\n",
    "CACHE_FILE = \"../../data/incremental_cache.json\"\n",
    "FED_SCRAPER_DIR = \"../../Fed-Scraper-main/fed_scraper\"\n",
    "FED_SCRAPER_MAIN_DIR = \"../../Fed-Scraper-main/\"\n",
    "DOCUMENTS_DIR = os.path.join(FED_SCRAPER_MAIN_DIR, \"data\", \"documents_by_type\")\n",
    "\n",
    "CURRENT_DIR = Path.cwd() \n",
    "FED_SCRAPER_ROOT_ABS = (CURRENT_DIR / FED_SCRAPER_DIR).resolve()\n",
    "print(FED_SCRAPER_ROOT_ABS)\n",
    "FED_SCRAPER_MAIN_DIR_ROOT = (CURRENT_DIR / FED_SCRAPER_MAIN_DIR).resolve()\n",
    "print(FED_SCRAPER_MAIN_DIR_ROOT)\n",
    "RAW_CSV_ABS = (CURRENT_DIR / RAW_CSV).resolve()\n",
    "CACHE_FILE_ABS = (CURRENT_DIR / CACHE_FILE).resolve()\n",
    "\n",
    "INCLUDED_KINDS = {\n",
    "    \"fomc\", \"minutes\", \"press_conferences\", \"speeches\",\n",
    "    \"transcripts\", \"projections\", \"beige_book\", \"redbooks\", \"teal_book\"\n",
    "}\n",
    "\n",
    "class FedSpeechFetcher:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "    \n",
    "    def run_spiders(self):\n",
    "        \"\"\"\n",
    "        Run all spiders (full scraping on first execution)\n",
    "        \"\"\"\n",
    "        spiders = [\n",
    "            \"beige_book_archive\",\n",
    "            \"beige_book_current\",\n",
    "            \"fomc_calendar\",\n",
    "            \"historical_materials\"\n",
    "        ]\n",
    "\n",
    "        for spider in spiders:\n",
    "            #subprocess.run(\n",
    "            #    [\"scrapy\", \"crawl\", spider],\n",
    "            #    cwd=FED_SCRAPER_DIR,\n",
    "            #    check=True)\n",
    "            # Capturing output is the key change!\n",
    "            scrapy_cmd = str(FED_SCRAPER_ROOT_ABS)\n",
    "            print(scrapy_cmd)\n",
    "            result = subprocess.run(\n",
    "                [\"scrapy\", \"crawl\", spider],\n",
    "                cwd=scrapy_cmd,\n",
    "                capture_output=True,  # Capture stdout and stderr\n",
    "                text=True,            # Decode output as text\n",
    "                check=False           # Temporarily set to False so it doesn't crash Python\n",
    "            )\n",
    "            \n",
    "            # If the process returned an error code, print the full traceback from Scrapy\n",
    "            if result.returncode != 0:\n",
    "                print(f\"ERROR: Scrapy spider '{spider}' failed with exit status {result.returncode}\")\n",
    "                print(\"\\n--- Scrapy STDOUT ---\")\n",
    "                print(result.stdout)\n",
    "                print(\"\\n--- Scrapy STDERR (Actual Error Message) ---\")\n",
    "                print(result.stderr)\n",
    "                \n",
    "                # Now raise the error manually so the calling function knows it failed\n",
    "                result.check_returncode()\n",
    "\n",
    "    def load_fed_scraper_outputs(self):\n",
    "        frames = []\n",
    "        DOCUMENTS_DIR_ABS = FED_SCRAPER_MAIN_DIR_ROOT / \"data\" / \"documents_by_type\"\n",
    "        print(\"Documents dir exists:\", DOCUMENTS_DIR_ABS ,DOCUMENTS_DIR_ABS.exists())\n",
    "        if not DOCUMENTS_DIR_ABS.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Fed-Scraper output directory not found: {DOCUMENTS_DIR_ABS}\\n\"\n",
    "                \"Make sure spiders completed successfully.\"\n",
    "            )\n",
    "            \n",
    "        for fname in DOCUMENTS_DIR_ABS.iterdir():\n",
    "            if not fname.name.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            kind = fname.stem\n",
    "\n",
    "            if kind not in INCLUDED_KINDS:\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(fname)\n",
    "            df[\"document_kind\"] = kind\n",
    "            frames.append(df)\n",
    "\n",
    "        if not frames:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if not os.path.exists(CACHE_FILE_ABS):\n",
    "            return {}\n",
    "        with open(CACHE_FILE_ABS, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def save_cache(self, cache):\n",
    "        Path(CACHE_FILE_ABS).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(CACHE_FILE_ABS, \"w\") as f:\n",
    "            json.dump(cache, f, indent=2)\n",
    "            \n",
    "    def append_new_speeches(self, df):\n",
    "        \"\"\"\n",
    "        write new rows to data/raw_speeches.csv\n",
    "        \"\"\"\n",
    "        cache = self.load_cache()\n",
    "\n",
    "        new_rows = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            uid = f\"{row['document_kind']}_{row['url']}\"\n",
    "\n",
    "            release = row.get(\"release_date\", None)\n",
    "            if isinstance(release, float):\n",
    "                release = None\n",
    "\n",
    "            if uid not in cache:\n",
    "                new_rows.append(row)\n",
    "                cache[uid] = release\n",
    "            else:\n",
    "                if release and release != cache[uid]:\n",
    "                    new_rows.append(row)\n",
    "                    cache[uid] = release\n",
    "\n",
    "        # append to disk\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            Path(RAW_CSV_ABS).parent.mkdir(parents=True, exist_ok=True) # Ensure parent directory exists\n",
    "\n",
    "            if not os.path.exists(RAW_CSV_ABS):\n",
    "                new_df.to_csv(RAW_CSV_ABS, index=False)\n",
    "            else:\n",
    "                old = pd.read_csv(RAW_CSV_ABS)\n",
    "                full = pd.concat([old, new_df], ignore_index=True)\n",
    "                full.to_csv(RAW_CSV_ABS, index=False)\n",
    "\n",
    "        self.save_cache(cache)\n",
    "\n",
    "    def run_full(self):\n",
    "        self.run_spiders()\n",
    "        df = self.load_fed_scraper_outputs()\n",
    "        self.append_new_speeches(df)\n",
    "        return df\n",
    "\n",
    "    def run_incremental(self):\n",
    "        df = self.load_fed_scraper_outputs()\n",
    "        self.append_new_speeches(df)\n",
    "        return df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fetcher = FedSpeechFetcher()\n",
    "\n",
    "if not os.path.exists(RAW_CSV):\n",
    "    print(\"Performing full scrape...\")\n",
    "    fetcher.run_full()\n",
    "else:\n",
    "    print(\"Performing incremental scrape...\")\n",
    "    fetcher.run_incremental()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4164f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# optional transformer backend\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Paths\n",
    "# ---------------------------\n",
    "RAW_CSV = Path(\"../../Fed-Scraper-main/data/fomc_documents.csv\").resolve()\n",
    "OUT_DIR = Path(\"../../data/processed\").resolve()\n",
    "DAILY_DIR = OUT_DIR / \"daily_embeddings\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DAILY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Document priors\n",
    "# ---------------------------\n",
    "DOC_PRIORS = {\n",
    "    \"fomc\":              {\"weight\": 1.00, \"half_life\": 10},\n",
    "    \"minutes\":           {\"weight\": 0.95, \"half_life\": 12},\n",
    "    \"press_conferences\": {\"weight\": 0.90, \"half_life\": 7},\n",
    "    \"transcripts\":       {\"weight\": 0.85, \"half_life\": 20},\n",
    "    \"projections\":       {\"weight\": 1.00, \"half_life\": 30},\n",
    "    \"speeches\":          {\"weight\": 0.60, \"half_life\": 5},\n",
    "    \"beige_book\":        {\"weight\": 0.45, \"half_life\": 4},\n",
    "    \"redbooks\":          {\"weight\": 0.35, \"half_life\": 3},\n",
    "    \"teal_book\":         {\"weight\": 0.50, \"half_life\": 15},\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Text utilities\n",
    "# ---------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def half_life_from_text(text: str, base: float) -> float:\n",
    "    keywords = [\n",
    "        \"uncertainty\", \"outlook\", \"forecast\", \"risks\",\n",
    "        \"expected\", \"projected\", \"anticipate\"\n",
    "    ]\n",
    "    boost = sum(1 for k in keywords if k in text)\n",
    "    return base * (1 + 0.15 * boost)\n",
    "\n",
    "\n",
    "def decay_weight(days: int, half_life: float) -> float:\n",
    "    return math.exp(-math.log(2) * days / half_life)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Builder\n",
    "# ---------------------------\n",
    "class SpeechDecayBuilder:\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(RAW_CSV)\n",
    "\n",
    "        self.df[\"release_date\"] = pd.to_datetime(\n",
    "            self.df[\"release_date\"], errors=\"coerce\"\n",
    "        )\n",
    "        self.df = self.df.dropna(subset=[\"release_date\"])\n",
    "\n",
    "        self.df[\"clean_text\"] = self.df[\"text\"].apply(clean_text)\n",
    "\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            self.embed_dim = self.embedder.get_sentence_embedding_dimension()\n",
    "            self.backend = \"sentence-transformers\"\n",
    "        else:\n",
    "            self.embedder = TfidfVectorizer(\n",
    "                max_features=512,\n",
    "                stop_words=\"english\"\n",
    "            )\n",
    "            self.backend = \"tfidf\"\n",
    "\n",
    "    # -----------------------\n",
    "    # Embeddings\n",
    "    # -----------------------\n",
    "    def build_embeddings(self):\n",
    "        texts = self.df[\"clean_text\"].tolist()\n",
    "\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            embeddings = self.embedder.encode(\n",
    "                texts, batch_size=32, show_progress_bar=True\n",
    "            )\n",
    "        else:\n",
    "            embeddings = self.embedder.fit_transform(texts).toarray()\n",
    "\n",
    "        np.save(OUT_DIR / \"embeddings.npy\", embeddings)\n",
    "\n",
    "        self.df[\"doc_index\"] = np.arange(len(self.df))\n",
    "        self.df.to_csv(OUT_DIR / \"speech_metadata.csv\", index=False)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    # -----------------------\n",
    "    # Daily aggregation\n",
    "    # -----------------------\n",
    "    def build_daily_embeddings(self, embeddings: np.ndarray):\n",
    "        start = self.df[\"release_date\"].min().date()\n",
    "        end = datetime.utcnow().date()\n",
    "\n",
    "        for day in pd.date_range(start, end):\n",
    "            vec = np.zeros(embeddings.shape[1])\n",
    "\n",
    "            for _, row in self.df.iterrows():\n",
    "                if row[\"release_date\"].date() > day.date():\n",
    "                    continue\n",
    "\n",
    "                days = (day.date() - row[\"release_date\"].date()).days\n",
    "                prior = DOC_PRIORS.get(row[\"document_kind\"], None)\n",
    "                if prior is None:\n",
    "                    continue\n",
    "\n",
    "                hl = half_life_from_text(\n",
    "                    row[\"clean_text\"], prior[\"half_life\"]\n",
    "                )\n",
    "                w = prior[\"weight\"] * decay_weight(days, hl)\n",
    "                vec += w * embeddings[int(row[\"doc_index\"])]\n",
    "\n",
    "            if np.linalg.norm(vec) > 0:\n",
    "                vec /= np.linalg.norm(vec)\n",
    "\n",
    "            out = DAILY_DIR / f\"{day.date()}_embeddings.npz\"\n",
    "            np.savez_compressed(\n",
    "                out,\n",
    "                embedding=vec,\n",
    "                date=str(day.date()),\n",
    "                backend=self.backend\n",
    "            )\n",
    "\n",
    "    # -----------------------\n",
    "    # Full run\n",
    "    # -----------------------\n",
    "    def run(self):\n",
    "        embeddings = self.build_embeddings()\n",
    "        self.build_daily_embeddings(embeddings)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# CLI\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    builder = SpeechDecayBuilder()\n",
    "    builder.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
