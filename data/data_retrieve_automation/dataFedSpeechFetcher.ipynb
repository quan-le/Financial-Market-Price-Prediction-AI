{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcf649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d2cd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\\fed_scraper\n",
      "C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\n",
      "Performing incremental scrape...\n",
      "Documents dir exists: C:\\Users\\windows\\OneDrive - VietNam National University - HCM INTERNATIONAL UNIVERSITY\\Desktop\\Programming\\Project\\Price-Prediction-AI\\Fed-Scraper-main\\data\\documents_by_type True\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "RAW_CSV = \"../../data/raw_speeches.csv\"\n",
    "CACHE_FILE = \"../../data/incremental_cache.json\"\n",
    "FED_SCRAPER_DIR = \"../../Fed-Scraper-main/fed_scraper\"\n",
    "FED_SCRAPER_MAIN_DIR = \"../../Fed-Scraper-main/\"\n",
    "DOCUMENTS_DIR = os.path.join(FED_SCRAPER_MAIN_DIR, \"data\", \"documents_by_type\")\n",
    "\n",
    "CURRENT_DIR = Path.cwd() \n",
    "FED_SCRAPER_ROOT_ABS = (CURRENT_DIR / FED_SCRAPER_DIR).resolve()\n",
    "print(FED_SCRAPER_ROOT_ABS)\n",
    "FED_SCRAPER_MAIN_DIR_ROOT = (CURRENT_DIR / FED_SCRAPER_MAIN_DIR).resolve()\n",
    "print(FED_SCRAPER_MAIN_DIR_ROOT)\n",
    "RAW_CSV_ABS = (CURRENT_DIR / RAW_CSV).resolve()\n",
    "CACHE_FILE_ABS = (CURRENT_DIR / CACHE_FILE).resolve()\n",
    "\n",
    "INCLUDED_KINDS = {\n",
    "    \"fomc\", \"minutes\", \"press_conferences\", \"speeches\",\n",
    "    \"transcripts\", \"projections\", \"beige_book\", \"redbooks\", \"teal_book\"\n",
    "}\n",
    "\n",
    "class FedSpeechFetcher:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "    \n",
    "    def run_spiders(self):\n",
    "        \"\"\"\n",
    "        Run all spiders (full scraping on first execution)\n",
    "        \"\"\"\n",
    "        spiders = [\n",
    "            \"beige_book_archive\",\n",
    "            \"beige_book_current\",\n",
    "            \"fomc_calendar\",\n",
    "            \"historical_materials\"\n",
    "        ]\n",
    "\n",
    "        for spider in spiders:\n",
    "            #subprocess.run(\n",
    "            #    [\"scrapy\", \"crawl\", spider],\n",
    "            #    cwd=FED_SCRAPER_DIR,\n",
    "            #    check=True)\n",
    "            # Capturing output is the key change!\n",
    "            scrapy_cmd = str(FED_SCRAPER_ROOT_ABS)\n",
    "            print(scrapy_cmd)\n",
    "            result = subprocess.run(\n",
    "                [\"scrapy\", \"crawl\", spider],\n",
    "                cwd=scrapy_cmd,\n",
    "                capture_output=True,  # Capture stdout and stderr\n",
    "                text=True,            # Decode output as text\n",
    "                check=False           # Temporarily set to False so it doesn't crash Python\n",
    "            )\n",
    "            \n",
    "            # If the process returned an error code, print the full traceback from Scrapy\n",
    "            if result.returncode != 0:\n",
    "                print(f\"ERROR: Scrapy spider '{spider}' failed with exit status {result.returncode}\")\n",
    "                print(\"\\n--- Scrapy STDOUT ---\")\n",
    "                print(result.stdout)\n",
    "                print(\"\\n--- Scrapy STDERR (Actual Error Message) ---\")\n",
    "                print(result.stderr)\n",
    "                \n",
    "                # Now raise the error manually so the calling function knows it failed\n",
    "                result.check_returncode()\n",
    "\n",
    "    def load_fed_scraper_outputs(self):\n",
    "        frames = []\n",
    "        DOCUMENTS_DIR_ABS = FED_SCRAPER_MAIN_DIR_ROOT / \"data\" / \"documents_by_type\"\n",
    "        print(\"Documents dir exists:\", DOCUMENTS_DIR_ABS ,DOCUMENTS_DIR_ABS.exists())\n",
    "        if not DOCUMENTS_DIR_ABS.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Fed-Scraper output directory not found: {DOCUMENTS_DIR_ABS}\\n\"\n",
    "                \"Make sure spiders completed successfully.\"\n",
    "            )\n",
    "            \n",
    "        for fname in DOCUMENTS_DIR_ABS.iterdir():\n",
    "            if not fname.name.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            kind = fname.stem\n",
    "\n",
    "            if kind not in INCLUDED_KINDS:\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(fname)\n",
    "            df[\"document_kind\"] = kind\n",
    "            frames.append(df)\n",
    "\n",
    "        if not frames:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if not os.path.exists(CACHE_FILE_ABS):\n",
    "            return {}\n",
    "        with open(CACHE_FILE_ABS, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def save_cache(self, cache):\n",
    "        Path(CACHE_FILE_ABS).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(CACHE_FILE_ABS, \"w\") as f:\n",
    "            json.dump(cache, f, indent=2)\n",
    "            \n",
    "    def append_new_speeches(self, df):\n",
    "        \"\"\"\n",
    "        write new rows to data/raw_speeches.csv\n",
    "        \"\"\"\n",
    "        cache = self.load_cache()\n",
    "\n",
    "        new_rows = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            uid = f\"{row['document_kind']}_{row['url']}\"\n",
    "\n",
    "            release = row.get(\"release_date\", None)\n",
    "            if isinstance(release, float):\n",
    "                release = None\n",
    "\n",
    "            if uid not in cache:\n",
    "                new_rows.append(row)\n",
    "                cache[uid] = release\n",
    "            else:\n",
    "                if release and release != cache[uid]:\n",
    "                    new_rows.append(row)\n",
    "                    cache[uid] = release\n",
    "\n",
    "        # append to disk\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            Path(RAW_CSV_ABS).parent.mkdir(parents=True, exist_ok=True) # Ensure parent directory exists\n",
    "\n",
    "            if not os.path.exists(RAW_CSV_ABS):\n",
    "                new_df.to_csv(RAW_CSV_ABS, index=False)\n",
    "            else:\n",
    "                old = pd.read_csv(RAW_CSV_ABS)\n",
    "                full = pd.concat([old, new_df], ignore_index=True)\n",
    "                full.to_csv(RAW_CSV_ABS, index=False)\n",
    "\n",
    "        self.save_cache(cache)\n",
    "\n",
    "    def run_full(self):\n",
    "        self.run_spiders()\n",
    "        df = self.load_fed_scraper_outputs()\n",
    "        self.append_new_speeches(df)\n",
    "        return df\n",
    "\n",
    "    def run_incremental(self):\n",
    "        df = self.load_fed_scraper_outputs()\n",
    "        self.append_new_speeches(df)\n",
    "        return df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fetcher = FedSpeechFetcher()\n",
    "\n",
    "if not os.path.exists(RAW_CSV):\n",
    "    print(\"Performing full scrape...\")\n",
    "    fetcher.run_full()\n",
    "else:\n",
    "    print(\"Performing incremental scrape...\")\n",
    "    fetcher.run_incremental()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40158f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
